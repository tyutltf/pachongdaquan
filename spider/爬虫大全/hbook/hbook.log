2019-08-05 21:49:06 [scrapy.utils.log] INFO: Scrapy 1.6.0 started (bot: hbook)
2019-08-05 21:49:06 [scrapy.utils.log] INFO: Versions: lxml 4.3.4.0, libxml2 2.9.5, cssselect 1.0.3, parsel 1.5.1, w3lib 1.20.0, Twisted 18.4.0, Python 3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 17:00:18) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Windows-10-10.0.17763-SP0
2019-08-05 21:49:06 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hbook', 'LOG_FILE': 'hbook.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'hbook.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['hbook.spiders']}
2019-08-05 21:49:06 [scrapy.extensions.telnet] INFO: Telnet Password: 2faddbeaf726f602
2019-08-05 21:49:06 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-05 21:49:08 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'hbook.rotate_useragent.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'hbook.middlewares.HbookDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-05 21:49:08 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'hbook.middlewares.HbookSpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-05 21:49:08 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-05 21:49:08 [scrapy.core.engine] INFO: Spider opened
2019-08-05 21:49:08 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-05 21:49:08 [diaosi] INFO: Spider opened: diaosi
2019-08-05 21:49:08 [py.warnings] WARNING: C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\offsite.py:61: URLWarning: allowed_domains accepts only domains, not URLs. Ignoring URL entry https://m.diaosixs.org in allowed_domains.
  warnings.warn(message, URLWarning)

2019-08-05 21:49:08 [diaosi] INFO: Spider opened: diaosi
2019-08-05 21:49:08 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-05 21:49:10 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-05 21:49:10 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 659,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 4374,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 1,
 'downloader/response_status_count/404': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 5, 13, 49, 10, 476778),
 'log_count/INFO': 11,
 'log_count/WARNING': 1,
 'response_received_count': 2,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/404': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2019, 8, 5, 13, 49, 8, 447307)}
2019-08-05 21:49:10 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-05 21:52:13 [scrapy.utils.log] INFO: Scrapy 1.6.0 started (bot: hbook)
2019-08-05 21:52:13 [scrapy.utils.log] INFO: Versions: lxml 4.3.4.0, libxml2 2.9.5, cssselect 1.0.3, parsel 1.5.1, w3lib 1.20.0, Twisted 18.4.0, Python 3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 17:00:18) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Windows-10-10.0.17763-SP0
2019-08-05 21:52:13 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hbook', 'LOG_FILE': 'hbook.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'hbook.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['hbook.spiders']}
2019-08-05 21:52:13 [scrapy.extensions.telnet] INFO: Telnet Password: a625be4a774ee20b
2019-08-05 21:52:13 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-05 21:52:14 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'hbook.rotate_useragent.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'hbook.middlewares.HbookDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-05 21:52:14 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'hbook.middlewares.HbookSpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-05 21:52:14 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-05 21:52:14 [scrapy.core.engine] INFO: Spider opened
2019-08-05 21:52:14 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-05 21:52:14 [diaosi] INFO: Spider opened: diaosi
2019-08-05 21:52:14 [py.warnings] WARNING: C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\offsite.py:61: URLWarning: allowed_domains accepts only domains, not URLs. Ignoring URL entry https://m.diaosixs.org in allowed_domains.
  warnings.warn(message, URLWarning)

2019-08-05 21:52:14 [diaosi] INFO: Spider opened: diaosi
2019-08-05 21:52:14 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-05 21:52:15 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-05 21:52:15 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 651,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 4383,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 1,
 'downloader/response_status_count/404': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 5, 13, 52, 15, 710819),
 'log_count/INFO': 11,
 'log_count/WARNING': 1,
 'response_received_count': 2,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/404': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2019, 8, 5, 13, 52, 14, 313803)}
2019-08-05 21:52:15 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-05 21:55:45 [scrapy.utils.log] INFO: Scrapy 1.6.0 started (bot: hbook)
2019-08-05 21:55:45 [scrapy.utils.log] INFO: Versions: lxml 4.3.4.0, libxml2 2.9.5, cssselect 1.0.3, parsel 1.5.1, w3lib 1.20.0, Twisted 18.4.0, Python 3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 17:00:18) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Windows-10-10.0.17763-SP0
2019-08-05 21:55:45 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hbook', 'LOG_FILE': 'hbook.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'hbook.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['hbook.spiders']}
2019-08-05 21:55:45 [scrapy.extensions.telnet] INFO: Telnet Password: 6f4f082eae7b025a
2019-08-05 21:55:45 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-05 21:55:46 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'hbook.rotate_useragent.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'hbook.middlewares.HbookDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-05 21:55:46 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'hbook.middlewares.HbookSpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-05 21:55:46 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-05 21:55:46 [scrapy.core.engine] INFO: Spider opened
2019-08-05 21:55:46 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-05 21:55:46 [diaosi] INFO: Spider opened: diaosi
2019-08-05 21:55:46 [py.warnings] WARNING: C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\offsite.py:61: URLWarning: allowed_domains accepts only domains, not URLs. Ignoring URL entry https://m.diaosixs.org in allowed_domains.
  warnings.warn(message, URLWarning)

2019-08-05 21:55:46 [diaosi] INFO: Spider opened: diaosi
2019-08-05 21:55:46 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-05 21:55:47 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-05 21:55:47 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 654,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 4378,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 1,
 'downloader/response_status_count/404': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 5, 13, 55, 47, 897357),
 'log_count/INFO': 11,
 'log_count/WARNING': 1,
 'response_received_count': 2,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/404': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2019, 8, 5, 13, 55, 46, 539114)}
2019-08-05 21:55:47 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-05 21:56:55 [scrapy.utils.log] INFO: Scrapy 1.6.0 started (bot: hbook)
2019-08-05 21:56:55 [scrapy.utils.log] INFO: Versions: lxml 4.3.4.0, libxml2 2.9.5, cssselect 1.0.3, parsel 1.5.1, w3lib 1.20.0, Twisted 18.4.0, Python 3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 17:00:18) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Windows-10-10.0.17763-SP0
2019-08-05 21:56:55 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hbook', 'LOG_FILE': 'hbook.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'hbook.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['hbook.spiders']}
2019-08-05 21:56:55 [scrapy.extensions.telnet] INFO: Telnet Password: fc97334b89380acb
2019-08-05 21:56:55 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-05 21:56:56 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'hbook.rotate_useragent.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'hbook.middlewares.HbookDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-05 21:56:56 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'hbook.middlewares.HbookSpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-05 21:56:56 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-05 21:56:56 [scrapy.core.engine] INFO: Spider opened
2019-08-05 21:56:56 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-05 21:56:56 [diaosi] INFO: Spider opened: diaosi
2019-08-05 21:56:56 [py.warnings] WARNING: C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\offsite.py:61: URLWarning: allowed_domains accepts only domains, not URLs. Ignoring URL entry https://m.diaosixs.org in allowed_domains.
  warnings.warn(message, URLWarning)

2019-08-05 21:56:56 [diaosi] INFO: Spider opened: diaosi
2019-08-05 21:56:56 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-05 21:56:57 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-05 21:56:57 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 654,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 4381,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 1,
 'downloader/response_status_count/404': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 5, 13, 56, 57, 822334),
 'log_count/INFO': 11,
 'log_count/WARNING': 1,
 'response_received_count': 2,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/404': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2019, 8, 5, 13, 56, 56, 513162)}
2019-08-05 21:56:57 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-05 22:21:00 [scrapy.utils.log] INFO: Scrapy 1.6.0 started (bot: hbook)
2019-08-05 22:21:00 [scrapy.utils.log] INFO: Versions: lxml 4.3.4.0, libxml2 2.9.5, cssselect 1.0.3, parsel 1.5.1, w3lib 1.20.0, Twisted 18.4.0, Python 3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 17:00:18) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Windows-10-10.0.17763-SP0
2019-08-05 22:21:00 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hbook', 'LOG_FILE': 'hbook.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'hbook.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['hbook.spiders']}
2019-08-05 22:21:00 [scrapy.extensions.telnet] INFO: Telnet Password: cedc417e7c9ee8cf
2019-08-05 22:21:00 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-05 22:21:01 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'hbook.rotate_useragent.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'hbook.middlewares.HbookDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-05 22:21:01 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'hbook.middlewares.HbookSpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-05 22:21:01 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-05 22:21:01 [scrapy.core.engine] INFO: Spider opened
2019-08-05 22:21:01 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-05 22:21:01 [diaosi] INFO: Spider opened: diaosi
2019-08-05 22:21:01 [py.warnings] WARNING: C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\offsite.py:61: URLWarning: allowed_domains accepts only domains, not URLs. Ignoring URL entry https://m.diaosixs.org in allowed_domains.
  warnings.warn(message, URLWarning)

2019-08-05 22:21:01 [diaosi] INFO: Spider opened: diaosi
2019-08-05 22:21:01 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-05 22:21:12 [scrapy.core.scraper] ERROR: Spider error processing <GET https://m.diaosixs.org/top/allvisit_1/> (referer: None)
Traceback (most recent call last):
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "G:\pythonAI\爬虫大全\hbook\hbook\middlewares.py", line 35, in process_spider_output
    for i in result:
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "G:\pythonAI\爬虫大全\hbook\hbook\spiders\diaosi.py", line 21, in parse
    item['bookname']=bookname
TypeError: 'ItemMeta' object does not support item assignment
2019-08-05 22:21:12 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-05 22:21:12 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 648,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 4376,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 1,
 'downloader/response_status_count/404': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 5, 14, 21, 12, 375714),
 'log_count/ERROR': 1,
 'log_count/INFO': 11,
 'log_count/WARNING': 1,
 'response_received_count': 2,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/404': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/TypeError': 1,
 'start_time': datetime.datetime(2019, 8, 5, 14, 21, 1, 710829)}
2019-08-05 22:21:12 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-05 22:22:34 [scrapy.utils.log] INFO: Scrapy 1.6.0 started (bot: hbook)
2019-08-05 22:22:34 [scrapy.utils.log] INFO: Versions: lxml 4.3.4.0, libxml2 2.9.5, cssselect 1.0.3, parsel 1.5.1, w3lib 1.20.0, Twisted 18.4.0, Python 3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 17:00:18) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Windows-10-10.0.17763-SP0
2019-08-05 22:22:34 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hbook', 'LOG_FILE': 'hbook.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'hbook.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['hbook.spiders']}
2019-08-05 22:22:35 [scrapy.extensions.telnet] INFO: Telnet Password: cbf6e565b325308b
2019-08-05 22:22:35 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-05 22:22:36 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'hbook.rotate_useragent.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'hbook.middlewares.HbookDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-05 22:22:36 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'hbook.middlewares.HbookSpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-05 22:22:36 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-05 22:22:36 [scrapy.core.engine] INFO: Spider opened
2019-08-05 22:22:36 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-05 22:22:36 [diaosi] INFO: Spider opened: diaosi
2019-08-05 22:22:36 [py.warnings] WARNING: C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\offsite.py:61: URLWarning: allowed_domains accepts only domains, not URLs. Ignoring URL entry https://m.diaosixs.org in allowed_domains.
  warnings.warn(message, URLWarning)

2019-08-05 22:22:36 [diaosi] INFO: Spider opened: diaosi
2019-08-05 22:22:36 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-05 22:22:37 [scrapy.core.scraper] ERROR: Spider error processing <GET https://m.diaosixs.org/top/allvisit_1/> (referer: None)
Traceback (most recent call last):
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "G:\pythonAI\爬虫大全\hbook\hbook\middlewares.py", line 35, in process_spider_output
    for i in result:
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "G:\pythonAI\爬虫大全\hbook\hbook\spiders\diaosi.py", line 21, in parse
    item['bookname']=bookname
TypeError: 'ItemMeta' object does not support item assignment
2019-08-05 22:22:37 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-05 22:22:37 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 649,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 4381,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 1,
 'downloader/response_status_count/404': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 5, 14, 22, 37, 541476),
 'log_count/ERROR': 1,
 'log_count/INFO': 11,
 'log_count/WARNING': 1,
 'response_received_count': 2,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/404': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/TypeError': 1,
 'start_time': datetime.datetime(2019, 8, 5, 14, 22, 36, 175161)}
2019-08-05 22:22:37 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-05 22:24:32 [scrapy.utils.log] INFO: Scrapy 1.6.0 started (bot: hbook)
2019-08-05 22:24:32 [scrapy.utils.log] INFO: Versions: lxml 4.3.4.0, libxml2 2.9.5, cssselect 1.0.3, parsel 1.5.1, w3lib 1.20.0, Twisted 18.4.0, Python 3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 17:00:18) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Windows-10-10.0.17763-SP0
2019-08-05 22:24:32 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hbook', 'LOG_FILE': 'hbook.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'hbook.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['hbook.spiders']}
2019-08-05 22:24:32 [scrapy.extensions.telnet] INFO: Telnet Password: 3a36be583d0b95ca
2019-08-05 22:24:32 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-05 22:24:33 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'hbook.rotate_useragent.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'hbook.middlewares.HbookDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-05 22:24:33 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'hbook.middlewares.HbookSpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-05 22:24:33 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-05 22:24:33 [scrapy.core.engine] INFO: Spider opened
2019-08-05 22:24:33 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-05 22:24:33 [diaosi] INFO: Spider opened: diaosi
2019-08-05 22:24:33 [py.warnings] WARNING: C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\offsite.py:61: URLWarning: allowed_domains accepts only domains, not URLs. Ignoring URL entry https://m.diaosixs.org in allowed_domains.
  warnings.warn(message, URLWarning)

2019-08-05 22:24:33 [diaosi] INFO: Spider opened: diaosi
2019-08-05 22:24:33 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-05 22:24:34 [scrapy.core.scraper] ERROR: Spider error processing <GET https://m.diaosixs.org/top/allvisit_1/> (referer: None)
Traceback (most recent call last):
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "G:\pythonAI\爬虫大全\hbook\hbook\middlewares.py", line 35, in process_spider_output
    for i in result:
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "G:\pythonAI\爬虫大全\hbook\hbook\spiders\diaosi.py", line 21, in parse
    item['bookname']=bookname
TypeError: 'ItemMeta' object does not support item assignment
2019-08-05 22:24:34 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-05 22:24:34 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 647,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 4383,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 1,
 'downloader/response_status_count/404': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 5, 14, 24, 34, 706454),
 'log_count/ERROR': 1,
 'log_count/INFO': 11,
 'log_count/WARNING': 1,
 'response_received_count': 2,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/404': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/TypeError': 1,
 'start_time': datetime.datetime(2019, 8, 5, 14, 24, 33, 371810)}
2019-08-05 22:24:34 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-05 22:26:10 [scrapy.utils.log] INFO: Scrapy 1.6.0 started (bot: hbook)
2019-08-05 22:26:10 [scrapy.utils.log] INFO: Versions: lxml 4.3.4.0, libxml2 2.9.5, cssselect 1.0.3, parsel 1.5.1, w3lib 1.20.0, Twisted 18.4.0, Python 3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 17:00:18) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Windows-10-10.0.17763-SP0
2019-08-05 22:26:10 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hbook', 'LOG_FILE': 'hbook.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'hbook.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['hbook.spiders']}
2019-08-05 22:26:10 [scrapy.extensions.telnet] INFO: Telnet Password: c5098ee842aca42a
2019-08-05 22:26:10 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-05 22:26:12 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'hbook.rotate_useragent.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'hbook.middlewares.HbookDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-05 22:26:12 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'hbook.middlewares.HbookSpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-05 22:26:12 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-05 22:26:12 [scrapy.core.engine] INFO: Spider opened
2019-08-05 22:26:12 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-05 22:26:12 [diaosi] INFO: Spider opened: diaosi
2019-08-05 22:26:12 [py.warnings] WARNING: C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\offsite.py:61: URLWarning: allowed_domains accepts only domains, not URLs. Ignoring URL entry https://m.diaosixs.org in allowed_domains.
  warnings.warn(message, URLWarning)

2019-08-05 22:26:12 [diaosi] INFO: Spider opened: diaosi
2019-08-05 22:26:12 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-05 22:26:15 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-05 22:26:15 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 9029,
 'downloader/request_count': 22,
 'downloader/request_method_count/GET': 22,
 'downloader/response_bytes': 49353,
 'downloader/response_count': 22,
 'downloader/response_status_count/200': 21,
 'downloader/response_status_count/404': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 5, 14, 26, 15, 640977),
 'log_count/INFO': 11,
 'log_count/WARNING': 1,
 'request_depth_max': 1,
 'response_received_count': 22,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/404': 1,
 'scheduler/dequeued': 21,
 'scheduler/dequeued/memory': 21,
 'scheduler/enqueued': 21,
 'scheduler/enqueued/memory': 21,
 'start_time': datetime.datetime(2019, 8, 5, 14, 26, 12, 154932)}
2019-08-05 22:26:15 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-05 22:26:54 [scrapy.utils.log] INFO: Scrapy 1.6.0 started (bot: hbook)
2019-08-05 22:26:54 [scrapy.utils.log] INFO: Versions: lxml 4.3.4.0, libxml2 2.9.5, cssselect 1.0.3, parsel 1.5.1, w3lib 1.20.0, Twisted 18.4.0, Python 3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 17:00:18) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Windows-10-10.0.17763-SP0
2019-08-05 22:26:54 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hbook', 'LOG_FILE': 'hbook.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'hbook.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['hbook.spiders']}
2019-08-05 22:26:54 [scrapy.extensions.telnet] INFO: Telnet Password: c198b19f92d2e9d6
2019-08-05 22:26:54 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-05 22:26:55 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'hbook.rotate_useragent.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'hbook.middlewares.HbookDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-05 22:26:55 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'hbook.middlewares.HbookSpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-05 22:26:55 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-05 22:26:55 [scrapy.core.engine] INFO: Spider opened
2019-08-05 22:26:55 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-05 22:26:55 [diaosi] INFO: Spider opened: diaosi
2019-08-05 22:26:55 [py.warnings] WARNING: C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\offsite.py:61: URLWarning: allowed_domains accepts only domains, not URLs. Ignoring URL entry https://m.diaosixs.org in allowed_domains.
  warnings.warn(message, URLWarning)

2019-08-05 22:26:55 [diaosi] INFO: Spider opened: diaosi
2019-08-05 22:26:55 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-05 22:26:59 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-05 22:26:59 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 8806,
 'downloader/request_count': 22,
 'downloader/request_method_count/GET': 22,
 'downloader/response_bytes': 49491,
 'downloader/response_count': 22,
 'downloader/response_status_count/200': 21,
 'downloader/response_status_count/404': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 5, 14, 26, 59, 124077),
 'log_count/INFO': 11,
 'log_count/WARNING': 1,
 'request_depth_max': 1,
 'response_received_count': 22,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/404': 1,
 'scheduler/dequeued': 21,
 'scheduler/dequeued/memory': 21,
 'scheduler/enqueued': 21,
 'scheduler/enqueued/memory': 21,
 'start_time': datetime.datetime(2019, 8, 5, 14, 26, 55, 634844)}
2019-08-05 22:26:59 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-05 22:30:17 [scrapy.utils.log] INFO: Scrapy 1.6.0 started (bot: hbook)
2019-08-05 22:30:17 [scrapy.utils.log] INFO: Versions: lxml 4.3.4.0, libxml2 2.9.5, cssselect 1.0.3, parsel 1.5.1, w3lib 1.20.0, Twisted 18.4.0, Python 3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 17:00:18) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Windows-10-10.0.17763-SP0
2019-08-05 22:30:17 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hbook', 'LOG_FILE': 'hbook.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'hbook.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['hbook.spiders']}
2019-08-05 22:30:17 [scrapy.extensions.telnet] INFO: Telnet Password: 9f6d0f11f0c4ba82
2019-08-05 22:30:17 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-05 22:30:18 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'hbook.rotate_useragent.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'hbook.middlewares.HbookDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-05 22:30:18 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'hbook.middlewares.HbookSpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-05 22:30:18 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-05 22:30:18 [scrapy.core.engine] INFO: Spider opened
2019-08-05 22:30:18 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-05 22:30:18 [diaosi] INFO: Spider opened: diaosi
2019-08-05 22:30:18 [py.warnings] WARNING: C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\offsite.py:61: URLWarning: allowed_domains accepts only domains, not URLs. Ignoring URL entry https://m.diaosixs.org in allowed_domains.
  warnings.warn(message, URLWarning)

2019-08-05 22:30:18 [diaosi] INFO: Spider opened: diaosi
2019-08-05 22:30:18 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-05 22:30:22 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-05 22:30:22 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 8893,
 'downloader/request_count': 22,
 'downloader/request_method_count/GET': 22,
 'downloader/response_bytes': 49352,
 'downloader/response_count': 22,
 'downloader/response_status_count/200': 21,
 'downloader/response_status_count/404': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 5, 14, 30, 22, 195510),
 'log_count/INFO': 11,
 'log_count/WARNING': 1,
 'request_depth_max': 1,
 'response_received_count': 22,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/404': 1,
 'scheduler/dequeued': 21,
 'scheduler/dequeued/memory': 21,
 'scheduler/enqueued': 21,
 'scheduler/enqueued/memory': 21,
 'start_time': datetime.datetime(2019, 8, 5, 14, 30, 18, 530666)}
2019-08-05 22:30:22 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-05 22:31:26 [scrapy.utils.log] INFO: Scrapy 1.6.0 started (bot: hbook)
2019-08-05 22:31:26 [scrapy.utils.log] INFO: Versions: lxml 4.3.4.0, libxml2 2.9.5, cssselect 1.0.3, parsel 1.5.1, w3lib 1.20.0, Twisted 18.4.0, Python 3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 17:00:18) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Windows-10-10.0.17763-SP0
2019-08-05 22:31:26 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hbook', 'LOG_FILE': 'hbook.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'hbook.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['hbook.spiders']}
2019-08-05 22:31:26 [scrapy.extensions.telnet] INFO: Telnet Password: 215fc8b2a2d7f093
2019-08-05 22:31:26 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-05 22:31:28 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'hbook.rotate_useragent.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'hbook.middlewares.HbookDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-05 22:31:28 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'hbook.middlewares.HbookSpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-05 22:31:28 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-05 22:31:28 [scrapy.core.engine] INFO: Spider opened
2019-08-05 22:31:28 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-05 22:31:28 [diaosi] INFO: Spider opened: diaosi
2019-08-05 22:31:28 [py.warnings] WARNING: C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\offsite.py:61: URLWarning: allowed_domains accepts only domains, not URLs. Ignoring URL entry https://m.diaosixs.org in allowed_domains.
  warnings.warn(message, URLWarning)

2019-08-05 22:31:28 [diaosi] INFO: Spider opened: diaosi
2019-08-05 22:31:28 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-05 22:31:31 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-05 22:31:31 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 9143,
 'downloader/request_count': 22,
 'downloader/request_method_count/GET': 22,
 'downloader/response_bytes': 49440,
 'downloader/response_count': 22,
 'downloader/response_status_count/200': 21,
 'downloader/response_status_count/404': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 5, 14, 31, 31, 516865),
 'log_count/INFO': 11,
 'log_count/WARNING': 1,
 'request_depth_max': 1,
 'response_received_count': 22,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/404': 1,
 'scheduler/dequeued': 21,
 'scheduler/dequeued/memory': 21,
 'scheduler/enqueued': 21,
 'scheduler/enqueued/memory': 21,
 'start_time': datetime.datetime(2019, 8, 5, 14, 31, 28, 166321)}
2019-08-05 22:31:31 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-05 22:33:00 [scrapy.utils.log] INFO: Scrapy 1.6.0 started (bot: hbook)
2019-08-05 22:33:00 [scrapy.utils.log] INFO: Versions: lxml 4.3.4.0, libxml2 2.9.5, cssselect 1.0.3, parsel 1.5.1, w3lib 1.20.0, Twisted 18.4.0, Python 3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 17:00:18) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Windows-10-10.0.17763-SP0
2019-08-05 22:33:00 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hbook', 'LOG_FILE': 'hbook.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'hbook.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['hbook.spiders']}
2019-08-05 22:33:00 [scrapy.extensions.telnet] INFO: Telnet Password: b9b80483c4d48732
2019-08-05 22:33:00 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-05 22:33:01 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'hbook.rotate_useragent.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'hbook.middlewares.HbookDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-05 22:33:01 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'hbook.middlewares.HbookSpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-05 22:33:01 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-05 22:33:01 [scrapy.core.engine] INFO: Spider opened
2019-08-05 22:33:01 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-05 22:33:01 [diaosi] INFO: Spider opened: diaosi
2019-08-05 22:33:01 [py.warnings] WARNING: C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\offsite.py:61: URLWarning: allowed_domains accepts only domains, not URLs. Ignoring URL entry https://m.diaosixs.org in allowed_domains.
  warnings.warn(message, URLWarning)

2019-08-05 22:33:01 [diaosi] INFO: Spider opened: diaosi
2019-08-05 22:33:01 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-05 22:33:04 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-05 22:33:04 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 8914,
 'downloader/request_count': 22,
 'downloader/request_method_count/GET': 22,
 'downloader/response_bytes': 49390,
 'downloader/response_count': 22,
 'downloader/response_status_count/200': 21,
 'downloader/response_status_count/404': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 5, 14, 33, 4, 943347),
 'log_count/INFO': 11,
 'log_count/WARNING': 1,
 'request_depth_max': 1,
 'response_received_count': 22,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/404': 1,
 'scheduler/dequeued': 21,
 'scheduler/dequeued/memory': 21,
 'scheduler/enqueued': 21,
 'scheduler/enqueued/memory': 21,
 'start_time': datetime.datetime(2019, 8, 5, 14, 33, 1, 424970)}
2019-08-05 22:33:04 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-05 22:34:09 [scrapy.utils.log] INFO: Scrapy 1.6.0 started (bot: hbook)
2019-08-05 22:34:09 [scrapy.utils.log] INFO: Versions: lxml 4.3.4.0, libxml2 2.9.5, cssselect 1.0.3, parsel 1.5.1, w3lib 1.20.0, Twisted 18.4.0, Python 3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 17:00:18) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Windows-10-10.0.17763-SP0
2019-08-05 22:34:09 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hbook', 'LOG_FILE': 'hbook.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'hbook.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['hbook.spiders']}
2019-08-05 22:34:09 [scrapy.extensions.telnet] INFO: Telnet Password: add7f8f607913abd
2019-08-05 22:34:09 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-05 22:34:10 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'hbook.rotate_useragent.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'hbook.middlewares.HbookDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-05 22:34:10 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'hbook.middlewares.HbookSpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-05 22:34:10 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-05 22:34:10 [scrapy.core.engine] INFO: Spider opened
2019-08-05 22:34:10 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-05 22:34:10 [diaosi] INFO: Spider opened: diaosi
2019-08-05 22:34:10 [py.warnings] WARNING: C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\offsite.py:61: URLWarning: allowed_domains accepts only domains, not URLs. Ignoring URL entry https://m.diaosixs.org in allowed_domains.
  warnings.warn(message, URLWarning)

2019-08-05 22:34:10 [diaosi] INFO: Spider opened: diaosi
2019-08-05 22:34:10 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-05 22:34:13 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-05 22:34:13 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 9021,
 'downloader/request_count': 22,
 'downloader/request_method_count/GET': 22,
 'downloader/response_bytes': 49417,
 'downloader/response_count': 22,
 'downloader/response_status_count/200': 21,
 'downloader/response_status_count/404': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 5, 14, 34, 13, 893403),
 'log_count/INFO': 11,
 'log_count/WARNING': 1,
 'request_depth_max': 1,
 'response_received_count': 22,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/404': 1,
 'scheduler/dequeued': 21,
 'scheduler/dequeued/memory': 21,
 'scheduler/enqueued': 21,
 'scheduler/enqueued/memory': 21,
 'start_time': datetime.datetime(2019, 8, 5, 14, 34, 10, 577197)}
2019-08-05 22:34:13 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-05 22:37:11 [scrapy.utils.log] INFO: Scrapy 1.6.0 started (bot: hbook)
2019-08-05 22:37:11 [scrapy.utils.log] INFO: Versions: lxml 4.3.4.0, libxml2 2.9.5, cssselect 1.0.3, parsel 1.5.1, w3lib 1.20.0, Twisted 18.4.0, Python 3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 17:00:18) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Windows-10-10.0.17763-SP0
2019-08-05 22:37:11 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hbook', 'LOG_FILE': 'hbook.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'hbook.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['hbook.spiders']}
2019-08-05 22:37:11 [scrapy.extensions.telnet] INFO: Telnet Password: 4bd52b567d5d9a8d
2019-08-05 22:37:11 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-05 22:37:12 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'hbook.rotate_useragent.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'hbook.middlewares.HbookDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-05 22:37:12 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'hbook.middlewares.HbookSpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-05 22:37:12 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-05 22:37:12 [scrapy.core.engine] INFO: Spider opened
2019-08-05 22:37:12 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-05 22:37:12 [diaosi] INFO: Spider opened: diaosi
2019-08-05 22:37:12 [py.warnings] WARNING: C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\offsite.py:61: URLWarning: allowed_domains accepts only domains, not URLs. Ignoring URL entry https://m.diaosixs.org in allowed_domains.
  warnings.warn(message, URLWarning)

2019-08-05 22:37:12 [diaosi] INFO: Spider opened: diaosi
2019-08-05 22:37:12 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-05 22:37:16 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-05 22:37:16 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 9049,
 'downloader/request_count': 22,
 'downloader/request_method_count/GET': 22,
 'downloader/response_bytes': 49404,
 'downloader/response_count': 22,
 'downloader/response_status_count/200': 21,
 'downloader/response_status_count/404': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 5, 14, 37, 16, 490242),
 'log_count/INFO': 11,
 'log_count/WARNING': 1,
 'request_depth_max': 1,
 'response_received_count': 22,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/404': 1,
 'scheduler/dequeued': 21,
 'scheduler/dequeued/memory': 21,
 'scheduler/enqueued': 21,
 'scheduler/enqueued/memory': 21,
 'start_time': datetime.datetime(2019, 8, 5, 14, 37, 12, 929849)}
2019-08-05 22:37:16 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-06 20:29:57 [scrapy.utils.log] INFO: Scrapy 1.6.0 started (bot: hbook)
2019-08-06 20:29:57 [scrapy.utils.log] INFO: Versions: lxml 4.3.4.0, libxml2 2.9.5, cssselect 1.0.3, parsel 1.5.1, w3lib 1.20.0, Twisted 18.4.0, Python 3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 17:00:18) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Windows-10-10.0.17763-SP0
2019-08-06 20:29:57 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hbook', 'LOG_FILE': 'hbook.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'hbook.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['hbook.spiders']}
2019-08-06 20:29:57 [scrapy.extensions.telnet] INFO: Telnet Password: 00e738c5610534c5
2019-08-06 20:29:57 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-06 20:29:58 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'hbook.rotate_useragent.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'hbook.middlewares.HbookDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-06 20:29:58 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'hbook.middlewares.HbookSpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-06 20:29:58 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-06 20:29:58 [scrapy.core.engine] INFO: Spider opened
2019-08-06 20:29:58 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-06 20:29:58 [diaosi] INFO: Spider opened: diaosi
2019-08-06 20:29:58 [py.warnings] WARNING: C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\offsite.py:61: URLWarning: allowed_domains accepts only domains, not URLs. Ignoring URL entry https://m.diaosixs.org in allowed_domains.
  warnings.warn(message, URLWarning)

2019-08-06 20:29:58 [diaosi] INFO: Spider opened: diaosi
2019-08-06 20:29:58 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-06 20:30:02 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-06 20:30:02 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 17906,
 'downloader/request_count': 42,
 'downloader/request_method_count/GET': 42,
 'downloader/response_bytes': 119235,
 'downloader/response_count': 42,
 'downloader/response_status_count/200': 41,
 'downloader/response_status_count/404': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 6, 12, 30, 2, 908020),
 'log_count/INFO': 11,
 'log_count/WARNING': 1,
 'request_depth_max': 2,
 'response_received_count': 42,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/404': 1,
 'scheduler/dequeued': 41,
 'scheduler/dequeued/memory': 41,
 'scheduler/enqueued': 41,
 'scheduler/enqueued/memory': 41,
 'start_time': datetime.datetime(2019, 8, 6, 12, 29, 58, 755554)}
2019-08-06 20:30:02 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-06 20:32:26 [scrapy.utils.log] INFO: Scrapy 1.6.0 started (bot: hbook)
2019-08-06 20:32:26 [scrapy.utils.log] INFO: Versions: lxml 4.3.4.0, libxml2 2.9.5, cssselect 1.0.3, parsel 1.5.1, w3lib 1.20.0, Twisted 18.4.0, Python 3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 17:00:18) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Windows-10-10.0.17763-SP0
2019-08-06 20:32:26 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hbook', 'LOG_FILE': 'hbook.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'hbook.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['hbook.spiders']}
2019-08-06 20:32:26 [scrapy.extensions.telnet] INFO: Telnet Password: 83cd6114cfd221d5
2019-08-06 20:32:26 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-06 20:32:27 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'hbook.rotate_useragent.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'hbook.middlewares.HbookDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-06 20:32:27 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'hbook.middlewares.HbookSpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-06 20:32:27 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-06 20:32:27 [scrapy.core.engine] INFO: Spider opened
2019-08-06 20:32:27 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-06 20:32:27 [diaosi] INFO: Spider opened: diaosi
2019-08-06 20:32:27 [py.warnings] WARNING: C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\offsite.py:61: URLWarning: allowed_domains accepts only domains, not URLs. Ignoring URL entry https://m.diaosixs.org in allowed_domains.
  warnings.warn(message, URLWarning)

2019-08-06 20:32:27 [diaosi] INFO: Spider opened: diaosi
2019-08-06 20:32:27 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-06 20:32:30 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-06 20:32:30 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 17656,
 'downloader/request_count': 42,
 'downloader/request_method_count/GET': 42,
 'downloader/response_bytes': 119141,
 'downloader/response_count': 42,
 'downloader/response_status_count/200': 41,
 'downloader/response_status_count/404': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 6, 12, 32, 30, 768676),
 'log_count/INFO': 11,
 'log_count/WARNING': 1,
 'request_depth_max': 2,
 'response_received_count': 42,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/404': 1,
 'scheduler/dequeued': 41,
 'scheduler/dequeued/memory': 41,
 'scheduler/enqueued': 41,
 'scheduler/enqueued/memory': 41,
 'start_time': datetime.datetime(2019, 8, 6, 12, 32, 27, 192284)}
2019-08-06 20:32:30 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-06 20:37:22 [scrapy.utils.log] INFO: Scrapy 1.6.0 started (bot: hbook)
2019-08-06 20:37:22 [scrapy.utils.log] INFO: Versions: lxml 4.3.4.0, libxml2 2.9.5, cssselect 1.0.3, parsel 1.5.1, w3lib 1.20.0, Twisted 18.4.0, Python 3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 17:00:18) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Windows-10-10.0.17763-SP0
2019-08-06 20:37:22 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hbook', 'LOG_FILE': 'hbook.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'hbook.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['hbook.spiders']}
2019-08-06 20:37:22 [scrapy.extensions.telnet] INFO: Telnet Password: f9834969d1fde21b
2019-08-06 20:37:22 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-06 20:37:23 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'hbook.rotate_useragent.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'hbook.middlewares.HbookDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-06 20:37:23 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'hbook.middlewares.HbookSpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-06 20:37:23 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-06 20:37:23 [scrapy.core.engine] INFO: Spider opened
2019-08-06 20:37:23 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-06 20:37:23 [diaosi] INFO: Spider opened: diaosi
2019-08-06 20:37:23 [py.warnings] WARNING: C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\offsite.py:61: URLWarning: allowed_domains accepts only domains, not URLs. Ignoring URL entry https://m.diaosixs.org in allowed_domains.
  warnings.warn(message, URLWarning)

2019-08-06 20:37:23 [diaosi] INFO: Spider opened: diaosi
2019-08-06 20:37:23 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-06 20:37:27 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-06 20:37:27 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 17771,
 'downloader/request_count': 42,
 'downloader/request_method_count/GET': 42,
 'downloader/response_bytes': 119206,
 'downloader/response_count': 42,
 'downloader/response_status_count/200': 41,
 'downloader/response_status_count/404': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 6, 12, 37, 27, 214506),
 'log_count/INFO': 11,
 'log_count/WARNING': 1,
 'request_depth_max': 2,
 'response_received_count': 42,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/404': 1,
 'scheduler/dequeued': 41,
 'scheduler/dequeued/memory': 41,
 'scheduler/enqueued': 41,
 'scheduler/enqueued/memory': 41,
 'start_time': datetime.datetime(2019, 8, 6, 12, 37, 23, 189707)}
2019-08-06 20:37:27 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-06 20:38:33 [scrapy.utils.log] INFO: Scrapy 1.6.0 started (bot: hbook)
2019-08-06 20:38:33 [scrapy.utils.log] INFO: Versions: lxml 4.3.4.0, libxml2 2.9.5, cssselect 1.0.3, parsel 1.5.1, w3lib 1.20.0, Twisted 18.4.0, Python 3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 17:00:18) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Windows-10-10.0.17763-SP0
2019-08-06 20:38:33 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hbook', 'LOG_FILE': 'hbook.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'hbook.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['hbook.spiders']}
2019-08-06 20:38:33 [scrapy.extensions.telnet] INFO: Telnet Password: 598b250125caad48
2019-08-06 20:38:33 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-06 20:38:34 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'hbook.rotate_useragent.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'hbook.middlewares.HbookDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-06 20:38:34 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'hbook.middlewares.HbookSpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-06 20:38:34 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-06 20:38:34 [scrapy.core.engine] INFO: Spider opened
2019-08-06 20:38:34 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-06 20:38:34 [diaosi] INFO: Spider opened: diaosi
2019-08-06 20:38:34 [py.warnings] WARNING: C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\offsite.py:61: URLWarning: allowed_domains accepts only domains, not URLs. Ignoring URL entry https://m.diaosixs.org in allowed_domains.
  warnings.warn(message, URLWarning)

2019-08-06 20:38:34 [diaosi] INFO: Spider opened: diaosi
2019-08-06 20:38:34 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-06 20:38:38 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-06 20:38:38 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 17911,
 'downloader/request_count': 42,
 'downloader/request_method_count/GET': 42,
 'downloader/response_bytes': 119063,
 'downloader/response_count': 42,
 'downloader/response_status_count/200': 41,
 'downloader/response_status_count/404': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 6, 12, 38, 38, 345282),
 'log_count/INFO': 11,
 'log_count/WARNING': 1,
 'request_depth_max': 2,
 'response_received_count': 42,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/404': 1,
 'scheduler/dequeued': 41,
 'scheduler/dequeued/memory': 41,
 'scheduler/enqueued': 41,
 'scheduler/enqueued/memory': 41,
 'start_time': datetime.datetime(2019, 8, 6, 12, 38, 34, 653732)}
2019-08-06 20:38:38 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-06 20:44:33 [scrapy.utils.log] INFO: Scrapy 1.6.0 started (bot: hbook)
2019-08-06 20:44:33 [scrapy.utils.log] INFO: Versions: lxml 4.3.4.0, libxml2 2.9.5, cssselect 1.0.3, parsel 1.5.1, w3lib 1.20.0, Twisted 18.4.0, Python 3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 17:00:18) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Windows-10-10.0.17763-SP0
2019-08-06 20:44:33 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hbook', 'LOG_FILE': 'hbook.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'hbook.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['hbook.spiders']}
2019-08-06 20:44:33 [scrapy.extensions.telnet] INFO: Telnet Password: 72bf157735aabeb9
2019-08-06 20:44:33 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-06 20:44:34 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'hbook.rotate_useragent.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'hbook.middlewares.HbookDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-06 20:44:34 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'hbook.middlewares.HbookSpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-06 20:44:34 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-06 20:44:34 [scrapy.core.engine] INFO: Spider opened
2019-08-06 20:44:34 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-06 20:44:34 [diaosi] INFO: Spider opened: diaosi
2019-08-06 20:44:34 [py.warnings] WARNING: C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\offsite.py:61: URLWarning: allowed_domains accepts only domains, not URLs. Ignoring URL entry https://m.diaosixs.org in allowed_domains.
  warnings.warn(message, URLWarning)

2019-08-06 20:44:34 [diaosi] INFO: Spider opened: diaosi
2019-08-06 20:44:34 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-06 20:44:37 [scrapy.core.scraper] ERROR: Spider error processing <GET https://m.diaosixs.org/0_655/138662.html> (referer: https://m.diaosixs.org//0_655/)
Traceback (most recent call last):
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "G:\pythonAI\爬虫大全\hbook\hbook\spiders\diaosi.py", line 42, in get_neirong
    xiaoshuo = re.sub(r'[\u4e00-\u9fa5]{1,}', '', xiaoshuo)
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\re.py", line 191, in sub
    return _compile(pattern, flags).sub(repl, string, count)
TypeError: expected string or bytes-like object
2019-08-06 20:44:37 [scrapy.core.scraper] ERROR: Spider error processing <GET https://m.diaosixs.org/0_354/73861.html> (referer: https://m.diaosixs.org//0_354/)
Traceback (most recent call last):
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "G:\pythonAI\爬虫大全\hbook\hbook\spiders\diaosi.py", line 42, in get_neirong
    xiaoshuo = re.sub(r'[\u4e00-\u9fa5]{1,}', '', xiaoshuo)
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\re.py", line 191, in sub
    return _compile(pattern, flags).sub(repl, string, count)
TypeError: expected string or bytes-like object
2019-08-06 20:44:38 [scrapy.core.scraper] ERROR: Spider error processing <GET https://m.diaosixs.org/0_378/76550.html> (referer: https://m.diaosixs.org//0_378/)
Traceback (most recent call last):
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "G:\pythonAI\爬虫大全\hbook\hbook\spiders\diaosi.py", line 42, in get_neirong
    xiaoshuo = re.sub(r'[\u4e00-\u9fa5]{1,}', '', xiaoshuo)
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\re.py", line 191, in sub
    return _compile(pattern, flags).sub(repl, string, count)
TypeError: expected string or bytes-like object
2019-08-06 20:44:38 [scrapy.core.scraper] ERROR: Spider error processing <GET https://m.diaosixs.org/0_32/11144.html> (referer: https://m.diaosixs.org//0_32/)
Traceback (most recent call last):
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "G:\pythonAI\爬虫大全\hbook\hbook\spiders\diaosi.py", line 42, in get_neirong
    xiaoshuo = re.sub(r'[\u4e00-\u9fa5]{1,}', '', xiaoshuo)
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\re.py", line 191, in sub
    return _compile(pattern, flags).sub(repl, string, count)
TypeError: expected string or bytes-like object
2019-08-06 20:44:38 [scrapy.core.scraper] ERROR: Spider error processing <GET https://m.diaosixs.org/1_1115/239734.html> (referer: https://m.diaosixs.org//1_1115/)
Traceback (most recent call last):
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "G:\pythonAI\爬虫大全\hbook\hbook\spiders\diaosi.py", line 42, in get_neirong
    xiaoshuo = re.sub(r'[\u4e00-\u9fa5]{1,}', '', xiaoshuo)
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\re.py", line 191, in sub
    return _compile(pattern, flags).sub(repl, string, count)
TypeError: expected string or bytes-like object
2019-08-06 20:44:38 [scrapy.core.scraper] ERROR: Spider error processing <GET https://m.diaosixs.org/0_46/13998.html> (referer: https://m.diaosixs.org//0_46/)
Traceback (most recent call last):
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "G:\pythonAI\爬虫大全\hbook\hbook\spiders\diaosi.py", line 42, in get_neirong
    xiaoshuo = re.sub(r'[\u4e00-\u9fa5]{1,}', '', xiaoshuo)
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\re.py", line 191, in sub
    return _compile(pattern, flags).sub(repl, string, count)
TypeError: expected string or bytes-like object
2019-08-06 20:44:38 [scrapy.core.scraper] ERROR: Spider error processing <GET https://m.diaosixs.org/0_27/10469.html> (referer: https://m.diaosixs.org//0_27/)
Traceback (most recent call last):
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "G:\pythonAI\爬虫大全\hbook\hbook\spiders\diaosi.py", line 42, in get_neirong
    xiaoshuo = re.sub(r'[\u4e00-\u9fa5]{1,}', '', xiaoshuo)
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\re.py", line 191, in sub
    return _compile(pattern, flags).sub(repl, string, count)
TypeError: expected string or bytes-like object
2019-08-06 20:44:38 [scrapy.core.scraper] ERROR: Spider error processing <GET https://m.diaosixs.org/0_132/34330.html> (referer: https://m.diaosixs.org//0_132/)
Traceback (most recent call last):
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "G:\pythonAI\爬虫大全\hbook\hbook\spiders\diaosi.py", line 42, in get_neirong
    xiaoshuo = re.sub(r'[\u4e00-\u9fa5]{1,}', '', xiaoshuo)
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\re.py", line 191, in sub
    return _compile(pattern, flags).sub(repl, string, count)
TypeError: expected string or bytes-like object
2019-08-06 20:44:38 [scrapy.core.scraper] ERROR: Spider error processing <GET https://m.diaosixs.org/0_571/116196.html> (referer: https://m.diaosixs.org//0_571/)
Traceback (most recent call last):
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "G:\pythonAI\爬虫大全\hbook\hbook\spiders\diaosi.py", line 42, in get_neirong
    xiaoshuo = re.sub(r'[\u4e00-\u9fa5]{1,}', '', xiaoshuo)
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\re.py", line 191, in sub
    return _compile(pattern, flags).sub(repl, string, count)
TypeError: expected string or bytes-like object
2019-08-06 20:44:38 [scrapy.core.scraper] ERROR: Spider error processing <GET https://m.diaosixs.org/0_742/151701.html> (referer: https://m.diaosixs.org//0_742/)
Traceback (most recent call last):
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "G:\pythonAI\爬虫大全\hbook\hbook\spiders\diaosi.py", line 42, in get_neirong
    xiaoshuo = re.sub(r'[\u4e00-\u9fa5]{1,}', '', xiaoshuo)
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\re.py", line 191, in sub
    return _compile(pattern, flags).sub(repl, string, count)
TypeError: expected string or bytes-like object
2019-08-06 20:44:38 [scrapy.core.scraper] ERROR: Spider error processing <GET https://m.diaosixs.org/1_1458/401523.html> (referer: https://m.diaosixs.org//1_1458/)
Traceback (most recent call last):
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "G:\pythonAI\爬虫大全\hbook\hbook\spiders\diaosi.py", line 42, in get_neirong
    xiaoshuo = re.sub(r'[\u4e00-\u9fa5]{1,}', '', xiaoshuo)
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\re.py", line 191, in sub
    return _compile(pattern, flags).sub(repl, string, count)
TypeError: expected string or bytes-like object
2019-08-06 20:44:38 [scrapy.core.scraper] ERROR: Spider error processing <GET https://m.diaosixs.org/1_1088/237732.html> (referer: https://m.diaosixs.org//1_1088/)
Traceback (most recent call last):
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "G:\pythonAI\爬虫大全\hbook\hbook\spiders\diaosi.py", line 42, in get_neirong
    xiaoshuo = re.sub(r'[\u4e00-\u9fa5]{1,}', '', xiaoshuo)
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\re.py", line 191, in sub
    return _compile(pattern, flags).sub(repl, string, count)
TypeError: expected string or bytes-like object
2019-08-06 20:44:38 [scrapy.core.scraper] ERROR: Spider error processing <GET https://m.diaosixs.org/0_28/10773.html> (referer: https://m.diaosixs.org//0_28/)
Traceback (most recent call last):
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "G:\pythonAI\爬虫大全\hbook\hbook\spiders\diaosi.py", line 42, in get_neirong
    xiaoshuo = re.sub(r'[\u4e00-\u9fa5]{1,}', '', xiaoshuo)
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\re.py", line 191, in sub
    return _compile(pattern, flags).sub(repl, string, count)
TypeError: expected string or bytes-like object
2019-08-06 20:44:38 [scrapy.core.scraper] ERROR: Spider error processing <GET https://m.diaosixs.org/0_14/5398.html> (referer: https://m.diaosixs.org//0_14/)
Traceback (most recent call last):
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "G:\pythonAI\爬虫大全\hbook\hbook\spiders\diaosi.py", line 42, in get_neirong
    xiaoshuo = re.sub(r'[\u4e00-\u9fa5]{1,}', '', xiaoshuo)
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\re.py", line 191, in sub
    return _compile(pattern, flags).sub(repl, string, count)
TypeError: expected string or bytes-like object
2019-08-06 20:44:38 [scrapy.core.scraper] ERROR: Spider error processing <GET https://m.diaosixs.org/0_572/116801.html> (referer: https://m.diaosixs.org//0_572/)
Traceback (most recent call last):
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "G:\pythonAI\爬虫大全\hbook\hbook\spiders\diaosi.py", line 42, in get_neirong
    xiaoshuo = re.sub(r'[\u4e00-\u9fa5]{1,}', '', xiaoshuo)
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\re.py", line 191, in sub
    return _compile(pattern, flags).sub(repl, string, count)
TypeError: expected string or bytes-like object
2019-08-06 20:44:38 [scrapy.core.scraper] ERROR: Spider error processing <GET https://m.diaosixs.org/1_1587/404213.html> (referer: https://m.diaosixs.org//1_1587/)
Traceback (most recent call last):
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "G:\pythonAI\爬虫大全\hbook\hbook\spiders\diaosi.py", line 42, in get_neirong
    xiaoshuo = re.sub(r'[\u4e00-\u9fa5]{1,}', '', xiaoshuo)
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\re.py", line 191, in sub
    return _compile(pattern, flags).sub(repl, string, count)
TypeError: expected string or bytes-like object
2019-08-06 20:44:38 [scrapy.core.scraper] ERROR: Spider error processing <GET https://m.diaosixs.org/0_15/5579.html> (referer: https://m.diaosixs.org//0_15/)
Traceback (most recent call last):
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "G:\pythonAI\爬虫大全\hbook\hbook\spiders\diaosi.py", line 42, in get_neirong
    xiaoshuo = re.sub(r'[\u4e00-\u9fa5]{1,}', '', xiaoshuo)
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\re.py", line 191, in sub
    return _compile(pattern, flags).sub(repl, string, count)
TypeError: expected string or bytes-like object
2019-08-06 20:44:38 [scrapy.core.scraper] ERROR: Spider error processing <GET https://m.diaosixs.org/0_78/20304.html> (referer: https://m.diaosixs.org//0_78/)
Traceback (most recent call last):
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "G:\pythonAI\爬虫大全\hbook\hbook\spiders\diaosi.py", line 42, in get_neirong
    xiaoshuo = re.sub(r'[\u4e00-\u9fa5]{1,}', '', xiaoshuo)
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\re.py", line 191, in sub
    return _compile(pattern, flags).sub(repl, string, count)
TypeError: expected string or bytes-like object
2019-08-06 20:44:38 [scrapy.core.scraper] ERROR: Spider error processing <GET https://m.diaosixs.org/0_1/1.html> (referer: https://m.diaosixs.org//0_1/)
Traceback (most recent call last):
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "G:\pythonAI\爬虫大全\hbook\hbook\spiders\diaosi.py", line 42, in get_neirong
    xiaoshuo = re.sub(r'[\u4e00-\u9fa5]{1,}', '', xiaoshuo)
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\re.py", line 191, in sub
    return _compile(pattern, flags).sub(repl, string, count)
TypeError: expected string or bytes-like object
2019-08-06 20:44:38 [scrapy.core.scraper] ERROR: Spider error processing <GET https://m.diaosixs.org/0_133/34344.html> (referer: https://m.diaosixs.org//0_133/)
Traceback (most recent call last):
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "G:\pythonAI\爬虫大全\hbook\hbook\spiders\diaosi.py", line 42, in get_neirong
    xiaoshuo = re.sub(r'[\u4e00-\u9fa5]{1,}', '', xiaoshuo)
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\re.py", line 191, in sub
    return _compile(pattern, flags).sub(repl, string, count)
TypeError: expected string or bytes-like object
2019-08-06 20:44:38 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-06 20:44:38 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 17883,
 'downloader/request_count': 42,
 'downloader/request_method_count/GET': 42,
 'downloader/response_bytes': 119127,
 'downloader/response_count': 42,
 'downloader/response_status_count/200': 41,
 'downloader/response_status_count/404': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 6, 12, 44, 38, 814441),
 'log_count/ERROR': 20,
 'log_count/INFO': 11,
 'log_count/WARNING': 1,
 'request_depth_max': 2,
 'response_received_count': 42,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/404': 1,
 'scheduler/dequeued': 41,
 'scheduler/dequeued/memory': 41,
 'scheduler/enqueued': 41,
 'scheduler/enqueued/memory': 41,
 'spider_exceptions/TypeError': 20,
 'start_time': datetime.datetime(2019, 8, 6, 12, 44, 34, 801629)}
2019-08-06 20:44:38 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-06 20:45:33 [scrapy.utils.log] INFO: Scrapy 1.6.0 started (bot: hbook)
2019-08-06 20:45:33 [scrapy.utils.log] INFO: Versions: lxml 4.3.4.0, libxml2 2.9.5, cssselect 1.0.3, parsel 1.5.1, w3lib 1.20.0, Twisted 18.4.0, Python 3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 17:00:18) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Windows-10-10.0.17763-SP0
2019-08-06 20:45:33 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hbook', 'LOG_FILE': 'hbook.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'hbook.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['hbook.spiders']}
2019-08-06 20:45:33 [scrapy.extensions.telnet] INFO: Telnet Password: beaebee5b12512b4
2019-08-06 20:45:33 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-06 20:45:34 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'hbook.rotate_useragent.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'hbook.middlewares.HbookDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-06 20:45:34 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'hbook.middlewares.HbookSpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-06 20:45:34 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-06 20:45:34 [scrapy.core.engine] INFO: Spider opened
2019-08-06 20:45:34 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-06 20:45:34 [diaosi] INFO: Spider opened: diaosi
2019-08-06 20:45:34 [py.warnings] WARNING: C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\offsite.py:61: URLWarning: allowed_domains accepts only domains, not URLs. Ignoring URL entry https://m.diaosixs.org in allowed_domains.
  warnings.warn(message, URLWarning)

2019-08-06 20:45:34 [diaosi] INFO: Spider opened: diaosi
2019-08-06 20:45:34 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-06 20:45:37 [scrapy.core.scraper] ERROR: Spider error processing <GET https://m.diaosixs.org/1_1458/401523.html> (referer: https://m.diaosixs.org//1_1458/)
Traceback (most recent call last):
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "G:\pythonAI\爬虫大全\hbook\hbook\spiders\diaosi.py", line 43, in get_neirong
    xiaoshuo = re.sub(r'[\u4e00-\u9fa5]{1,}', '', xiaoshuo)
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\re.py", line 191, in sub
    return _compile(pattern, flags).sub(repl, string, count)
TypeError: expected string or bytes-like object
2019-08-06 20:45:37 [scrapy.core.scraper] ERROR: Spider error processing <GET https://m.diaosixs.org/0_572/116801.html> (referer: https://m.diaosixs.org//0_572/)
Traceback (most recent call last):
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "G:\pythonAI\爬虫大全\hbook\hbook\spiders\diaosi.py", line 43, in get_neirong
    xiaoshuo = re.sub(r'[\u4e00-\u9fa5]{1,}', '', xiaoshuo)
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\re.py", line 191, in sub
    return _compile(pattern, flags).sub(repl, string, count)
TypeError: expected string or bytes-like object
2019-08-06 20:45:38 [scrapy.core.scraper] ERROR: Spider error processing <GET https://m.diaosixs.org/0_28/10773.html> (referer: https://m.diaosixs.org//0_28/)
Traceback (most recent call last):
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "G:\pythonAI\爬虫大全\hbook\hbook\spiders\diaosi.py", line 43, in get_neirong
    xiaoshuo = re.sub(r'[\u4e00-\u9fa5]{1,}', '', xiaoshuo)
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\re.py", line 191, in sub
    return _compile(pattern, flags).sub(repl, string, count)
TypeError: expected string or bytes-like object
2019-08-06 20:45:38 [scrapy.core.scraper] ERROR: Spider error processing <GET https://m.diaosixs.org/1_1088/237732.html> (referer: https://m.diaosixs.org//1_1088/)
Traceback (most recent call last):
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "G:\pythonAI\爬虫大全\hbook\hbook\spiders\diaosi.py", line 43, in get_neirong
    xiaoshuo = re.sub(r'[\u4e00-\u9fa5]{1,}', '', xiaoshuo)
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\re.py", line 191, in sub
    return _compile(pattern, flags).sub(repl, string, count)
TypeError: expected string or bytes-like object
2019-08-06 20:45:38 [scrapy.core.scraper] ERROR: Spider error processing <GET https://m.diaosixs.org/0_378/76550.html> (referer: https://m.diaosixs.org//0_378/)
Traceback (most recent call last):
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "G:\pythonAI\爬虫大全\hbook\hbook\spiders\diaosi.py", line 43, in get_neirong
    xiaoshuo = re.sub(r'[\u4e00-\u9fa5]{1,}', '', xiaoshuo)
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\re.py", line 191, in sub
    return _compile(pattern, flags).sub(repl, string, count)
TypeError: expected string or bytes-like object
2019-08-06 20:45:38 [scrapy.core.scraper] ERROR: Spider error processing <GET https://m.diaosixs.org/0_354/73861.html> (referer: https://m.diaosixs.org//0_354/)
Traceback (most recent call last):
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "G:\pythonAI\爬虫大全\hbook\hbook\spiders\diaosi.py", line 43, in get_neirong
    xiaoshuo = re.sub(r'[\u4e00-\u9fa5]{1,}', '', xiaoshuo)
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\re.py", line 191, in sub
    return _compile(pattern, flags).sub(repl, string, count)
TypeError: expected string or bytes-like object
2019-08-06 20:45:38 [scrapy.core.scraper] ERROR: Spider error processing <GET https://m.diaosixs.org/0_571/116196.html> (referer: https://m.diaosixs.org//0_571/)
Traceback (most recent call last):
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "G:\pythonAI\爬虫大全\hbook\hbook\spiders\diaosi.py", line 43, in get_neirong
    xiaoshuo = re.sub(r'[\u4e00-\u9fa5]{1,}', '', xiaoshuo)
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\re.py", line 191, in sub
    return _compile(pattern, flags).sub(repl, string, count)
TypeError: expected string or bytes-like object
2019-08-06 20:45:38 [scrapy.core.scraper] ERROR: Spider error processing <GET https://m.diaosixs.org/0_78/20304.html> (referer: https://m.diaosixs.org//0_78/)
Traceback (most recent call last):
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "G:\pythonAI\爬虫大全\hbook\hbook\spiders\diaosi.py", line 43, in get_neirong
    xiaoshuo = re.sub(r'[\u4e00-\u9fa5]{1,}', '', xiaoshuo)
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\re.py", line 191, in sub
    return _compile(pattern, flags).sub(repl, string, count)
TypeError: expected string or bytes-like object
2019-08-06 20:45:38 [scrapy.core.scraper] ERROR: Spider error processing <GET https://m.diaosixs.org/1_1587/404213.html> (referer: https://m.diaosixs.org//1_1587/)
Traceback (most recent call last):
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "G:\pythonAI\爬虫大全\hbook\hbook\spiders\diaosi.py", line 43, in get_neirong
    xiaoshuo = re.sub(r'[\u4e00-\u9fa5]{1,}', '', xiaoshuo)
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\re.py", line 191, in sub
    return _compile(pattern, flags).sub(repl, string, count)
TypeError: expected string or bytes-like object
2019-08-06 20:45:38 [scrapy.core.scraper] ERROR: Spider error processing <GET https://m.diaosixs.org/0_14/5398.html> (referer: https://m.diaosixs.org//0_14/)
Traceback (most recent call last):
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "G:\pythonAI\爬虫大全\hbook\hbook\spiders\diaosi.py", line 43, in get_neirong
    xiaoshuo = re.sub(r'[\u4e00-\u9fa5]{1,}', '', xiaoshuo)
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\re.py", line 191, in sub
    return _compile(pattern, flags).sub(repl, string, count)
TypeError: expected string or bytes-like object
2019-08-06 20:45:38 [scrapy.core.scraper] ERROR: Spider error processing <GET https://m.diaosixs.org/0_15/5579.html> (referer: https://m.diaosixs.org//0_15/)
Traceback (most recent call last):
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "G:\pythonAI\爬虫大全\hbook\hbook\spiders\diaosi.py", line 43, in get_neirong
    xiaoshuo = re.sub(r'[\u4e00-\u9fa5]{1,}', '', xiaoshuo)
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\re.py", line 191, in sub
    return _compile(pattern, flags).sub(repl, string, count)
TypeError: expected string or bytes-like object
2019-08-06 20:45:38 [scrapy.core.scraper] ERROR: Spider error processing <GET https://m.diaosixs.org/0_133/34344.html> (referer: https://m.diaosixs.org//0_133/)
Traceback (most recent call last):
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "G:\pythonAI\爬虫大全\hbook\hbook\spiders\diaosi.py", line 43, in get_neirong
    xiaoshuo = re.sub(r'[\u4e00-\u9fa5]{1,}', '', xiaoshuo)
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\re.py", line 191, in sub
    return _compile(pattern, flags).sub(repl, string, count)
TypeError: expected string or bytes-like object
2019-08-06 20:45:38 [scrapy.core.scraper] ERROR: Spider error processing <GET https://m.diaosixs.org/0_1/1.html> (referer: https://m.diaosixs.org//0_1/)
Traceback (most recent call last):
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "G:\pythonAI\爬虫大全\hbook\hbook\spiders\diaosi.py", line 43, in get_neirong
    xiaoshuo = re.sub(r'[\u4e00-\u9fa5]{1,}', '', xiaoshuo)
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\re.py", line 191, in sub
    return _compile(pattern, flags).sub(repl, string, count)
TypeError: expected string or bytes-like object
2019-08-06 20:45:38 [scrapy.core.scraper] ERROR: Spider error processing <GET https://m.diaosixs.org/0_132/34330.html> (referer: https://m.diaosixs.org//0_132/)
Traceback (most recent call last):
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "G:\pythonAI\爬虫大全\hbook\hbook\spiders\diaosi.py", line 43, in get_neirong
    xiaoshuo = re.sub(r'[\u4e00-\u9fa5]{1,}', '', xiaoshuo)
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\re.py", line 191, in sub
    return _compile(pattern, flags).sub(repl, string, count)
TypeError: expected string or bytes-like object
2019-08-06 20:45:38 [scrapy.core.scraper] ERROR: Spider error processing <GET https://m.diaosixs.org/0_742/151701.html> (referer: https://m.diaosixs.org//0_742/)
Traceback (most recent call last):
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "G:\pythonAI\爬虫大全\hbook\hbook\spiders\diaosi.py", line 43, in get_neirong
    xiaoshuo = re.sub(r'[\u4e00-\u9fa5]{1,}', '', xiaoshuo)
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\re.py", line 191, in sub
    return _compile(pattern, flags).sub(repl, string, count)
TypeError: expected string or bytes-like object
2019-08-06 20:45:38 [scrapy.core.scraper] ERROR: Spider error processing <GET https://m.diaosixs.org/1_1115/239734.html> (referer: https://m.diaosixs.org//1_1115/)
Traceback (most recent call last):
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "G:\pythonAI\爬虫大全\hbook\hbook\spiders\diaosi.py", line 43, in get_neirong
    xiaoshuo = re.sub(r'[\u4e00-\u9fa5]{1,}', '', xiaoshuo)
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\re.py", line 191, in sub
    return _compile(pattern, flags).sub(repl, string, count)
TypeError: expected string or bytes-like object
2019-08-06 20:45:38 [scrapy.core.scraper] ERROR: Spider error processing <GET https://m.diaosixs.org/0_655/138662.html> (referer: https://m.diaosixs.org//0_655/)
Traceback (most recent call last):
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "G:\pythonAI\爬虫大全\hbook\hbook\spiders\diaosi.py", line 43, in get_neirong
    xiaoshuo = re.sub(r'[\u4e00-\u9fa5]{1,}', '', xiaoshuo)
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\re.py", line 191, in sub
    return _compile(pattern, flags).sub(repl, string, count)
TypeError: expected string or bytes-like object
2019-08-06 20:45:38 [scrapy.core.scraper] ERROR: Spider error processing <GET https://m.diaosixs.org/0_27/10469.html> (referer: https://m.diaosixs.org//0_27/)
Traceback (most recent call last):
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "G:\pythonAI\爬虫大全\hbook\hbook\spiders\diaosi.py", line 43, in get_neirong
    xiaoshuo = re.sub(r'[\u4e00-\u9fa5]{1,}', '', xiaoshuo)
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\re.py", line 191, in sub
    return _compile(pattern, flags).sub(repl, string, count)
TypeError: expected string or bytes-like object
2019-08-06 20:45:38 [scrapy.core.scraper] ERROR: Spider error processing <GET https://m.diaosixs.org/0_32/11144.html> (referer: https://m.diaosixs.org//0_32/)
Traceback (most recent call last):
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "G:\pythonAI\爬虫大全\hbook\hbook\spiders\diaosi.py", line 43, in get_neirong
    xiaoshuo = re.sub(r'[\u4e00-\u9fa5]{1,}', '', xiaoshuo)
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\re.py", line 191, in sub
    return _compile(pattern, flags).sub(repl, string, count)
TypeError: expected string or bytes-like object
2019-08-06 20:45:38 [scrapy.core.scraper] ERROR: Spider error processing <GET https://m.diaosixs.org/0_46/13998.html> (referer: https://m.diaosixs.org//0_46/)
Traceback (most recent call last):
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "G:\pythonAI\爬虫大全\hbook\hbook\spiders\diaosi.py", line 43, in get_neirong
    xiaoshuo = re.sub(r'[\u4e00-\u9fa5]{1,}', '', xiaoshuo)
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\re.py", line 191, in sub
    return _compile(pattern, flags).sub(repl, string, count)
TypeError: expected string or bytes-like object
2019-08-06 20:45:38 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-06 20:45:38 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 18139,
 'downloader/request_count': 42,
 'downloader/request_method_count/GET': 42,
 'downloader/response_bytes': 119263,
 'downloader/response_count': 42,
 'downloader/response_status_count/200': 41,
 'downloader/response_status_count/404': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 6, 12, 45, 38, 864626),
 'log_count/ERROR': 20,
 'log_count/INFO': 11,
 'log_count/WARNING': 1,
 'request_depth_max': 2,
 'response_received_count': 42,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/404': 1,
 'scheduler/dequeued': 41,
 'scheduler/dequeued/memory': 41,
 'scheduler/enqueued': 41,
 'scheduler/enqueued/memory': 41,
 'spider_exceptions/TypeError': 20,
 'start_time': datetime.datetime(2019, 8, 6, 12, 45, 34, 864626)}
2019-08-06 20:45:38 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-06 20:56:31 [scrapy.utils.log] INFO: Scrapy 1.6.0 started (bot: hbook)
2019-08-06 20:56:31 [scrapy.utils.log] INFO: Versions: lxml 4.3.4.0, libxml2 2.9.5, cssselect 1.0.3, parsel 1.5.1, w3lib 1.20.0, Twisted 18.4.0, Python 3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 17:00:18) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Windows-10-10.0.17763-SP0
2019-08-06 20:56:31 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hbook', 'LOG_FILE': 'hbook.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'hbook.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['hbook.spiders']}
2019-08-06 20:56:31 [scrapy.extensions.telnet] INFO: Telnet Password: 91aa6d952745c4e0
2019-08-06 20:56:31 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-06 20:56:32 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'hbook.rotate_useragent.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'hbook.middlewares.HbookDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-06 20:56:32 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'hbook.middlewares.HbookSpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-06 20:56:32 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-06 20:56:32 [scrapy.core.engine] INFO: Spider opened
2019-08-06 20:56:32 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-06 20:56:32 [diaosi] INFO: Spider opened: diaosi
2019-08-06 20:56:32 [py.warnings] WARNING: C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\offsite.py:61: URLWarning: allowed_domains accepts only domains, not URLs. Ignoring URL entry https://m.diaosixs.org in allowed_domains.
  warnings.warn(message, URLWarning)

2019-08-06 20:56:32 [diaosi] INFO: Spider opened: diaosi
2019-08-06 20:56:32 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-06 20:56:36 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-06 20:56:36 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 17797,
 'downloader/request_count': 42,
 'downloader/request_method_count/GET': 42,
 'downloader/response_bytes': 119127,
 'downloader/response_count': 42,
 'downloader/response_status_count/200': 41,
 'downloader/response_status_count/404': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 6, 12, 56, 36, 630952),
 'log_count/INFO': 11,
 'log_count/WARNING': 1,
 'request_depth_max': 2,
 'response_received_count': 42,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/404': 1,
 'scheduler/dequeued': 41,
 'scheduler/dequeued/memory': 41,
 'scheduler/enqueued': 41,
 'scheduler/enqueued/memory': 41,
 'start_time': datetime.datetime(2019, 8, 6, 12, 56, 32, 902779)}
2019-08-06 20:56:36 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-06 21:17:25 [scrapy.utils.log] INFO: Scrapy 1.6.0 started (bot: hbook)
2019-08-06 21:17:25 [scrapy.utils.log] INFO: Versions: lxml 4.3.4.0, libxml2 2.9.5, cssselect 1.0.3, parsel 1.5.1, w3lib 1.20.0, Twisted 18.4.0, Python 3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 17:00:18) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Windows-10-10.0.17763-SP0
2019-08-06 21:17:25 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hbook', 'LOG_FILE': 'hbook.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'hbook.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['hbook.spiders']}
2019-08-06 21:17:25 [scrapy.extensions.telnet] INFO: Telnet Password: a4c0273c591497d6
2019-08-06 21:17:25 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-06 21:17:27 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'hbook.rotate_useragent.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'hbook.middlewares.HbookDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-06 21:17:27 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'hbook.middlewares.HbookSpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-06 21:17:27 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-06 21:17:27 [scrapy.core.engine] INFO: Spider opened
2019-08-06 21:17:27 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-06 21:17:27 [diaosi] INFO: Spider opened: diaosi
2019-08-06 21:17:27 [py.warnings] WARNING: C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\offsite.py:61: URLWarning: allowed_domains accepts only domains, not URLs. Ignoring URL entry https://m.diaosixs.org in allowed_domains.
  warnings.warn(message, URLWarning)

2019-08-06 21:17:27 [diaosi] INFO: Spider opened: diaosi
2019-08-06 21:17:27 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-06 21:17:30 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-06 21:17:30 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 18059,
 'downloader/request_count': 42,
 'downloader/request_method_count/GET': 42,
 'downloader/response_bytes': 119137,
 'downloader/response_count': 42,
 'downloader/response_status_count/200': 41,
 'downloader/response_status_count/404': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 6, 13, 17, 30, 958127),
 'log_count/INFO': 11,
 'log_count/WARNING': 1,
 'request_depth_max': 2,
 'response_received_count': 42,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/404': 1,
 'scheduler/dequeued': 41,
 'scheduler/dequeued/memory': 41,
 'scheduler/enqueued': 41,
 'scheduler/enqueued/memory': 41,
 'start_time': datetime.datetime(2019, 8, 6, 13, 17, 27, 29204)}
2019-08-06 21:17:30 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-06 21:34:10 [scrapy.utils.log] INFO: Scrapy 1.6.0 started (bot: hbook)
2019-08-06 21:34:10 [scrapy.utils.log] INFO: Versions: lxml 4.3.4.0, libxml2 2.9.5, cssselect 1.0.3, parsel 1.5.1, w3lib 1.20.0, Twisted 18.4.0, Python 3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 17:00:18) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Windows-10-10.0.17763-SP0
2019-08-06 21:34:10 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hbook', 'LOG_FILE': 'hbook.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'hbook.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['hbook.spiders']}
2019-08-06 21:34:10 [scrapy.extensions.telnet] INFO: Telnet Password: 07e6021055bef8c7
2019-08-06 21:34:10 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-06 21:34:11 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'hbook.rotate_useragent.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'hbook.middlewares.HbookDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-06 21:34:11 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'hbook.middlewares.HbookSpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-06 21:34:11 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-06 21:34:11 [scrapy.core.engine] INFO: Spider opened
2019-08-06 21:34:11 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-06 21:34:11 [diaosi] INFO: Spider opened: diaosi
2019-08-06 21:34:11 [py.warnings] WARNING: C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\offsite.py:61: URLWarning: allowed_domains accepts only domains, not URLs. Ignoring URL entry https://m.diaosixs.org in allowed_domains.
  warnings.warn(message, URLWarning)

2019-08-06 21:34:11 [diaosi] INFO: Spider opened: diaosi
2019-08-06 21:34:11 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-06 21:34:16 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-06 21:34:16 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 18107,
 'downloader/request_count': 42,
 'downloader/request_method_count/GET': 42,
 'downloader/response_bytes': 119058,
 'downloader/response_count': 42,
 'downloader/response_status_count/200': 41,
 'downloader/response_status_count/404': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 6, 13, 34, 16, 451947),
 'log_count/INFO': 11,
 'log_count/WARNING': 1,
 'request_depth_max': 2,
 'response_received_count': 42,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/404': 1,
 'scheduler/dequeued': 41,
 'scheduler/dequeued/memory': 41,
 'scheduler/enqueued': 41,
 'scheduler/enqueued/memory': 41,
 'start_time': datetime.datetime(2019, 8, 6, 13, 34, 11, 874754)}
2019-08-06 21:34:16 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-06 21:34:56 [scrapy.utils.log] INFO: Scrapy 1.6.0 started (bot: hbook)
2019-08-06 21:34:56 [scrapy.utils.log] INFO: Versions: lxml 4.3.4.0, libxml2 2.9.5, cssselect 1.0.3, parsel 1.5.1, w3lib 1.20.0, Twisted 18.4.0, Python 3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 17:00:18) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Windows-10-10.0.17763-SP0
2019-08-06 21:34:56 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hbook', 'LOG_FILE': 'hbook.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'hbook.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['hbook.spiders']}
2019-08-06 21:34:56 [scrapy.extensions.telnet] INFO: Telnet Password: 757fc247f4b05b61
2019-08-06 21:34:56 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-06 21:34:57 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'hbook.rotate_useragent.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'hbook.middlewares.HbookDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-06 21:34:57 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'hbook.middlewares.HbookSpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-06 21:34:57 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-06 21:34:57 [scrapy.core.engine] INFO: Spider opened
2019-08-06 21:34:57 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-06 21:34:58 [diaosi] INFO: Spider opened: diaosi
2019-08-06 21:34:58 [py.warnings] WARNING: C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\offsite.py:61: URLWarning: allowed_domains accepts only domains, not URLs. Ignoring URL entry https://m.diaosixs.org in allowed_domains.
  warnings.warn(message, URLWarning)

2019-08-06 21:34:58 [diaosi] INFO: Spider opened: diaosi
2019-08-06 21:34:58 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-06 21:35:02 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-06 21:35:02 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 17899,
 'downloader/request_count': 42,
 'downloader/request_method_count/GET': 42,
 'downloader/response_bytes': 119200,
 'downloader/response_count': 42,
 'downloader/response_status_count/200': 41,
 'downloader/response_status_count/404': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 6, 13, 35, 2, 9703),
 'log_count/INFO': 11,
 'log_count/WARNING': 1,
 'request_depth_max': 2,
 'response_received_count': 42,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/404': 1,
 'scheduler/dequeued': 41,
 'scheduler/dequeued/memory': 41,
 'scheduler/enqueued': 41,
 'scheduler/enqueued/memory': 41,
 'start_time': datetime.datetime(2019, 8, 6, 13, 34, 57, 999989)}
2019-08-06 21:35:02 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-06 21:36:25 [scrapy.utils.log] INFO: Scrapy 1.6.0 started (bot: hbook)
2019-08-06 21:36:25 [scrapy.utils.log] INFO: Versions: lxml 4.3.4.0, libxml2 2.9.5, cssselect 1.0.3, parsel 1.5.1, w3lib 1.20.0, Twisted 18.4.0, Python 3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 17:00:18) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Windows-10-10.0.17763-SP0
2019-08-06 21:36:25 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hbook', 'LOG_FILE': 'hbook.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'hbook.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['hbook.spiders']}
2019-08-06 21:36:25 [scrapy.extensions.telnet] INFO: Telnet Password: ffb2d17a2df3deda
2019-08-06 21:36:25 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-06 21:36:27 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'hbook.rotate_useragent.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'hbook.middlewares.HbookDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-06 21:36:27 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'hbook.middlewares.HbookSpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-06 21:36:27 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-06 21:36:27 [scrapy.core.engine] INFO: Spider opened
2019-08-06 21:36:27 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-06 21:36:27 [diaosi] INFO: Spider opened: diaosi
2019-08-06 21:36:27 [py.warnings] WARNING: C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\offsite.py:61: URLWarning: allowed_domains accepts only domains, not URLs. Ignoring URL entry https://m.diaosixs.org in allowed_domains.
  warnings.warn(message, URLWarning)

2019-08-06 21:36:27 [diaosi] INFO: Spider opened: diaosi
2019-08-06 21:36:27 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-06 21:36:32 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-06 21:36:32 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 18320,
 'downloader/request_count': 42,
 'downloader/request_method_count/GET': 42,
 'downloader/response_bytes': 119141,
 'downloader/response_count': 42,
 'downloader/response_status_count/200': 41,
 'downloader/response_status_count/404': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 6, 13, 36, 32, 65746),
 'log_count/INFO': 11,
 'log_count/WARNING': 1,
 'request_depth_max': 2,
 'response_received_count': 42,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/404': 1,
 'scheduler/dequeued': 41,
 'scheduler/dequeued/memory': 41,
 'scheduler/enqueued': 41,
 'scheduler/enqueued/memory': 41,
 'start_time': datetime.datetime(2019, 8, 6, 13, 36, 27, 626750)}
2019-08-06 21:36:32 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-06 21:47:02 [scrapy.utils.log] INFO: Scrapy 1.6.0 started (bot: hbook)
2019-08-06 21:47:02 [scrapy.utils.log] INFO: Versions: lxml 4.3.4.0, libxml2 2.9.5, cssselect 1.0.3, parsel 1.5.1, w3lib 1.20.0, Twisted 18.4.0, Python 3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 17:00:18) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Windows-10-10.0.17763-SP0
2019-08-06 21:47:02 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hbook', 'LOG_FILE': 'hbook.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'hbook.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['hbook.spiders']}
2019-08-06 21:47:02 [scrapy.extensions.telnet] INFO: Telnet Password: 06dc136af108d60e
2019-08-06 21:47:02 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-06 21:47:03 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'hbook.rotate_useragent.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'hbook.middlewares.HbookDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-06 21:47:03 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'hbook.middlewares.HbookSpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-06 21:47:03 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-06 21:47:03 [scrapy.core.engine] INFO: Spider opened
2019-08-06 21:47:03 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-06 21:47:03 [diaosi] INFO: Spider opened: diaosi
2019-08-06 21:47:03 [py.warnings] WARNING: C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\offsite.py:61: URLWarning: allowed_domains accepts only domains, not URLs. Ignoring URL entry https://m.diaosixs.org in allowed_domains.
  warnings.warn(message, URLWarning)

2019-08-06 21:47:03 [diaosi] INFO: Spider opened: diaosi
2019-08-06 21:47:03 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-06 21:47:07 [scrapy.core.scraper] ERROR: Spider error processing <GET https://m.diaosixs.org/1_1088/237732.html> (referer: https://m.diaosixs.org//1_1088/)
Traceback (most recent call last):
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "G:\pythonAI\爬虫大全\hbook\hbook\middlewares.py", line 35, in process_spider_output
    for i in result:
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "G:\pythonAI\爬虫大全\hbook\hbook\spiders\diaosi.py", line 47, in get_neirong
    next_page_url='https://m.diaosixs.org'+next_page
TypeError: must be str, not list
2019-08-06 21:47:07 [scrapy.core.scraper] ERROR: Spider error processing <GET https://m.diaosixs.org/0_133/34344.html> (referer: https://m.diaosixs.org//0_133/)
Traceback (most recent call last):
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "G:\pythonAI\爬虫大全\hbook\hbook\middlewares.py", line 35, in process_spider_output
    for i in result:
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "G:\pythonAI\爬虫大全\hbook\hbook\spiders\diaosi.py", line 47, in get_neirong
    next_page_url='https://m.diaosixs.org'+next_page
TypeError: must be str, not list
2019-08-06 21:47:08 [scrapy.core.scraper] ERROR: Spider error processing <GET https://m.diaosixs.org/0_655/138662.html> (referer: https://m.diaosixs.org//0_655/)
Traceback (most recent call last):
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "G:\pythonAI\爬虫大全\hbook\hbook\middlewares.py", line 35, in process_spider_output
    for i in result:
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "G:\pythonAI\爬虫大全\hbook\hbook\spiders\diaosi.py", line 47, in get_neirong
    next_page_url='https://m.diaosixs.org'+next_page
TypeError: must be str, not list
2019-08-06 21:47:08 [scrapy.core.scraper] ERROR: Spider error processing <GET https://m.diaosixs.org/1_1587/404213.html> (referer: https://m.diaosixs.org//1_1587/)
Traceback (most recent call last):
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "G:\pythonAI\爬虫大全\hbook\hbook\middlewares.py", line 35, in process_spider_output
    for i in result:
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "G:\pythonAI\爬虫大全\hbook\hbook\spiders\diaosi.py", line 47, in get_neirong
    next_page_url='https://m.diaosixs.org'+next_page
TypeError: must be str, not list
2019-08-06 21:47:08 [scrapy.core.scraper] ERROR: Spider error processing <GET https://m.diaosixs.org/0_1/1.html> (referer: https://m.diaosixs.org//0_1/)
Traceback (most recent call last):
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "G:\pythonAI\爬虫大全\hbook\hbook\middlewares.py", line 35, in process_spider_output
    for i in result:
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "G:\pythonAI\爬虫大全\hbook\hbook\spiders\diaosi.py", line 47, in get_neirong
    next_page_url='https://m.diaosixs.org'+next_page
TypeError: must be str, not list
2019-08-06 21:47:08 [scrapy.core.scraper] ERROR: Spider error processing <GET https://m.diaosixs.org/0_28/10773.html> (referer: https://m.diaosixs.org//0_28/)
Traceback (most recent call last):
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "G:\pythonAI\爬虫大全\hbook\hbook\middlewares.py", line 35, in process_spider_output
    for i in result:
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "G:\pythonAI\爬虫大全\hbook\hbook\spiders\diaosi.py", line 47, in get_neirong
    next_page_url='https://m.diaosixs.org'+next_page
TypeError: must be str, not list
2019-08-06 21:47:08 [scrapy.core.scraper] ERROR: Spider error processing <GET https://m.diaosixs.org/0_14/5398.html> (referer: https://m.diaosixs.org//0_14/)
Traceback (most recent call last):
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "G:\pythonAI\爬虫大全\hbook\hbook\middlewares.py", line 35, in process_spider_output
    for i in result:
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "G:\pythonAI\爬虫大全\hbook\hbook\spiders\diaosi.py", line 47, in get_neirong
    next_page_url='https://m.diaosixs.org'+next_page
TypeError: must be str, not list
2019-08-06 21:47:08 [scrapy.core.scraper] ERROR: Spider error processing <GET https://m.diaosixs.org/0_132/34330.html> (referer: https://m.diaosixs.org//0_132/)
Traceback (most recent call last):
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "G:\pythonAI\爬虫大全\hbook\hbook\middlewares.py", line 35, in process_spider_output
    for i in result:
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "G:\pythonAI\爬虫大全\hbook\hbook\spiders\diaosi.py", line 47, in get_neirong
    next_page_url='https://m.diaosixs.org'+next_page
TypeError: must be str, not list
2019-08-06 21:47:08 [scrapy.core.scraper] ERROR: Spider error processing <GET https://m.diaosixs.org/0_27/10469.html> (referer: https://m.diaosixs.org//0_27/)
Traceback (most recent call last):
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "G:\pythonAI\爬虫大全\hbook\hbook\middlewares.py", line 35, in process_spider_output
    for i in result:
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "G:\pythonAI\爬虫大全\hbook\hbook\spiders\diaosi.py", line 47, in get_neirong
    next_page_url='https://m.diaosixs.org'+next_page
TypeError: must be str, not list
2019-08-06 21:47:08 [scrapy.core.scraper] ERROR: Spider error processing <GET https://m.diaosixs.org/0_15/5579.html> (referer: https://m.diaosixs.org//0_15/)
Traceback (most recent call last):
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "G:\pythonAI\爬虫大全\hbook\hbook\middlewares.py", line 35, in process_spider_output
    for i in result:
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "G:\pythonAI\爬虫大全\hbook\hbook\spiders\diaosi.py", line 47, in get_neirong
    next_page_url='https://m.diaosixs.org'+next_page
TypeError: must be str, not list
2019-08-06 21:47:08 [scrapy.core.scraper] ERROR: Spider error processing <GET https://m.diaosixs.org/1_1458/401523.html> (referer: https://m.diaosixs.org//1_1458/)
Traceback (most recent call last):
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "G:\pythonAI\爬虫大全\hbook\hbook\middlewares.py", line 35, in process_spider_output
    for i in result:
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "G:\pythonAI\爬虫大全\hbook\hbook\spiders\diaosi.py", line 47, in get_neirong
    next_page_url='https://m.diaosixs.org'+next_page
TypeError: must be str, not list
2019-08-06 21:47:08 [scrapy.core.scraper] ERROR: Spider error processing <GET https://m.diaosixs.org/0_78/20304.html> (referer: https://m.diaosixs.org//0_78/)
Traceback (most recent call last):
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "G:\pythonAI\爬虫大全\hbook\hbook\middlewares.py", line 35, in process_spider_output
    for i in result:
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "G:\pythonAI\爬虫大全\hbook\hbook\spiders\diaosi.py", line 47, in get_neirong
    next_page_url='https://m.diaosixs.org'+next_page
TypeError: must be str, not list
2019-08-06 21:47:08 [scrapy.core.scraper] ERROR: Spider error processing <GET https://m.diaosixs.org/1_1115/239734.html> (referer: https://m.diaosixs.org//1_1115/)
Traceback (most recent call last):
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "G:\pythonAI\爬虫大全\hbook\hbook\middlewares.py", line 35, in process_spider_output
    for i in result:
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "G:\pythonAI\爬虫大全\hbook\hbook\spiders\diaosi.py", line 47, in get_neirong
    next_page_url='https://m.diaosixs.org'+next_page
TypeError: must be str, not list
2019-08-06 21:47:08 [scrapy.core.scraper] ERROR: Spider error processing <GET https://m.diaosixs.org/0_742/151701.html> (referer: https://m.diaosixs.org//0_742/)
Traceback (most recent call last):
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "G:\pythonAI\爬虫大全\hbook\hbook\middlewares.py", line 35, in process_spider_output
    for i in result:
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "G:\pythonAI\爬虫大全\hbook\hbook\spiders\diaosi.py", line 47, in get_neirong
    next_page_url='https://m.diaosixs.org'+next_page
TypeError: must be str, not list
2019-08-06 21:47:08 [scrapy.core.scraper] ERROR: Spider error processing <GET https://m.diaosixs.org/0_572/116801.html> (referer: https://m.diaosixs.org//0_572/)
Traceback (most recent call last):
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "G:\pythonAI\爬虫大全\hbook\hbook\middlewares.py", line 35, in process_spider_output
    for i in result:
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "G:\pythonAI\爬虫大全\hbook\hbook\spiders\diaosi.py", line 47, in get_neirong
    next_page_url='https://m.diaosixs.org'+next_page
TypeError: must be str, not list
2019-08-06 21:47:08 [scrapy.core.scraper] ERROR: Spider error processing <GET https://m.diaosixs.org/0_46/13998.html> (referer: https://m.diaosixs.org//0_46/)
Traceback (most recent call last):
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "G:\pythonAI\爬虫大全\hbook\hbook\middlewares.py", line 35, in process_spider_output
    for i in result:
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "G:\pythonAI\爬虫大全\hbook\hbook\spiders\diaosi.py", line 47, in get_neirong
    next_page_url='https://m.diaosixs.org'+next_page
TypeError: must be str, not list
2019-08-06 21:47:08 [scrapy.core.scraper] ERROR: Spider error processing <GET https://m.diaosixs.org/0_32/11144.html> (referer: https://m.diaosixs.org//0_32/)
Traceback (most recent call last):
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "G:\pythonAI\爬虫大全\hbook\hbook\middlewares.py", line 35, in process_spider_output
    for i in result:
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "G:\pythonAI\爬虫大全\hbook\hbook\spiders\diaosi.py", line 47, in get_neirong
    next_page_url='https://m.diaosixs.org'+next_page
TypeError: must be str, not list
2019-08-06 21:47:08 [scrapy.core.scraper] ERROR: Spider error processing <GET https://m.diaosixs.org/0_354/73861.html> (referer: https://m.diaosixs.org//0_354/)
Traceback (most recent call last):
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "G:\pythonAI\爬虫大全\hbook\hbook\middlewares.py", line 35, in process_spider_output
    for i in result:
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "G:\pythonAI\爬虫大全\hbook\hbook\spiders\diaosi.py", line 47, in get_neirong
    next_page_url='https://m.diaosixs.org'+next_page
TypeError: must be str, not list
2019-08-06 21:47:08 [scrapy.core.scraper] ERROR: Spider error processing <GET https://m.diaosixs.org/0_571/116196.html> (referer: https://m.diaosixs.org//0_571/)
Traceback (most recent call last):
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "G:\pythonAI\爬虫大全\hbook\hbook\middlewares.py", line 35, in process_spider_output
    for i in result:
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "G:\pythonAI\爬虫大全\hbook\hbook\spiders\diaosi.py", line 47, in get_neirong
    next_page_url='https://m.diaosixs.org'+next_page
TypeError: must be str, not list
2019-08-06 21:47:08 [scrapy.core.scraper] ERROR: Spider error processing <GET https://m.diaosixs.org/0_378/76550.html> (referer: https://m.diaosixs.org//0_378/)
Traceback (most recent call last):
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "G:\pythonAI\爬虫大全\hbook\hbook\middlewares.py", line 35, in process_spider_output
    for i in result:
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "G:\pythonAI\爬虫大全\hbook\hbook\spiders\diaosi.py", line 47, in get_neirong
    next_page_url='https://m.diaosixs.org'+next_page
TypeError: must be str, not list
2019-08-06 21:47:08 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-06 21:47:08 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 17815,
 'downloader/request_count': 42,
 'downloader/request_method_count/GET': 42,
 'downloader/response_bytes': 119128,
 'downloader/response_count': 42,
 'downloader/response_status_count/200': 41,
 'downloader/response_status_count/404': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 6, 13, 47, 8, 922476),
 'log_count/ERROR': 20,
 'log_count/INFO': 11,
 'log_count/WARNING': 1,
 'request_depth_max': 2,
 'response_received_count': 42,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/404': 1,
 'scheduler/dequeued': 41,
 'scheduler/dequeued/memory': 41,
 'scheduler/enqueued': 41,
 'scheduler/enqueued/memory': 41,
 'spider_exceptions/TypeError': 20,
 'start_time': datetime.datetime(2019, 8, 6, 13, 47, 3, 647688)}
2019-08-06 21:47:08 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-06 21:49:10 [scrapy.utils.log] INFO: Scrapy 1.6.0 started (bot: hbook)
2019-08-06 21:49:10 [scrapy.utils.log] INFO: Versions: lxml 4.3.4.0, libxml2 2.9.5, cssselect 1.0.3, parsel 1.5.1, w3lib 1.20.0, Twisted 18.4.0, Python 3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 17:00:18) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Windows-10-10.0.17763-SP0
2019-08-06 21:49:10 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hbook', 'LOG_FILE': 'hbook.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'hbook.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['hbook.spiders']}
2019-08-06 21:49:10 [scrapy.extensions.telnet] INFO: Telnet Password: 11469638b095ef6d
2019-08-06 21:49:10 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-06 21:49:12 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'hbook.rotate_useragent.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'hbook.middlewares.HbookDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-06 21:49:12 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'hbook.middlewares.HbookSpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-06 21:49:12 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-06 21:49:12 [scrapy.core.engine] INFO: Spider opened
2019-08-06 21:49:12 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-06 21:49:12 [diaosi] INFO: Spider opened: diaosi
2019-08-06 21:49:12 [py.warnings] WARNING: C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\offsite.py:61: URLWarning: allowed_domains accepts only domains, not URLs. Ignoring URL entry https://m.diaosixs.org in allowed_domains.
  warnings.warn(message, URLWarning)

2019-08-06 21:49:12 [diaosi] INFO: Spider opened: diaosi
2019-08-06 21:49:12 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-06 21:49:16 [scrapy.core.scraper] ERROR: Spider error processing <GET https://m.diaosixs.org/0_655/138662.html> (referer: https://m.diaosixs.org//0_655/)
Traceback (most recent call last):
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "G:\pythonAI\爬虫大全\hbook\hbook\middlewares.py", line 35, in process_spider_output
    for i in result:
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "G:\pythonAI\爬虫大全\hbook\hbook\spiders\diaosi.py", line 47, in get_neirong
    next_page_url='https://m.diaosixs.org'+next_page
TypeError: must be str, not list
2019-08-06 21:49:16 [scrapy.core.scraper] ERROR: Spider error processing <GET https://m.diaosixs.org/0_354/73861.html> (referer: https://m.diaosixs.org//0_354/)
Traceback (most recent call last):
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "G:\pythonAI\爬虫大全\hbook\hbook\middlewares.py", line 35, in process_spider_output
    for i in result:
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "G:\pythonAI\爬虫大全\hbook\hbook\spiders\diaosi.py", line 47, in get_neirong
    next_page_url='https://m.diaosixs.org'+next_page
TypeError: must be str, not list
2019-08-06 21:49:16 [scrapy.core.scraper] ERROR: Spider error processing <GET https://m.diaosixs.org/0_27/10469.html> (referer: https://m.diaosixs.org//0_27/)
Traceback (most recent call last):
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "G:\pythonAI\爬虫大全\hbook\hbook\middlewares.py", line 35, in process_spider_output
    for i in result:
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "G:\pythonAI\爬虫大全\hbook\hbook\spiders\diaosi.py", line 47, in get_neirong
    next_page_url='https://m.diaosixs.org'+next_page
TypeError: must be str, not list
2019-08-06 21:49:16 [scrapy.core.scraper] ERROR: Spider error processing <GET https://m.diaosixs.org/0_132/34330.html> (referer: https://m.diaosixs.org//0_132/)
Traceback (most recent call last):
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "G:\pythonAI\爬虫大全\hbook\hbook\middlewares.py", line 35, in process_spider_output
    for i in result:
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "G:\pythonAI\爬虫大全\hbook\hbook\spiders\diaosi.py", line 47, in get_neirong
    next_page_url='https://m.diaosixs.org'+next_page
TypeError: must be str, not list
2019-08-06 21:49:16 [scrapy.core.scraper] ERROR: Spider error processing <GET https://m.diaosixs.org/0_32/11144.html> (referer: https://m.diaosixs.org//0_32/)
Traceback (most recent call last):
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "G:\pythonAI\爬虫大全\hbook\hbook\middlewares.py", line 35, in process_spider_output
    for i in result:
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "G:\pythonAI\爬虫大全\hbook\hbook\spiders\diaosi.py", line 47, in get_neirong
    next_page_url='https://m.diaosixs.org'+next_page
TypeError: must be str, not list
2019-08-06 21:49:16 [scrapy.core.scraper] ERROR: Spider error processing <GET https://m.diaosixs.org/0_742/151701.html> (referer: https://m.diaosixs.org//0_742/)
Traceback (most recent call last):
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "G:\pythonAI\爬虫大全\hbook\hbook\middlewares.py", line 35, in process_spider_output
    for i in result:
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "G:\pythonAI\爬虫大全\hbook\hbook\spiders\diaosi.py", line 47, in get_neirong
    next_page_url='https://m.diaosixs.org'+next_page
TypeError: must be str, not list
2019-08-06 21:49:17 [scrapy.core.scraper] ERROR: Spider error processing <GET https://m.diaosixs.org/0_378/76550.html> (referer: https://m.diaosixs.org//0_378/)
Traceback (most recent call last):
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "G:\pythonAI\爬虫大全\hbook\hbook\middlewares.py", line 35, in process_spider_output
    for i in result:
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "G:\pythonAI\爬虫大全\hbook\hbook\spiders\diaosi.py", line 47, in get_neirong
    next_page_url='https://m.diaosixs.org'+next_page
TypeError: must be str, not list
2019-08-06 21:49:17 [scrapy.core.scraper] ERROR: Spider error processing <GET https://m.diaosixs.org/0_46/13998.html> (referer: https://m.diaosixs.org//0_46/)
Traceback (most recent call last):
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "G:\pythonAI\爬虫大全\hbook\hbook\middlewares.py", line 35, in process_spider_output
    for i in result:
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "G:\pythonAI\爬虫大全\hbook\hbook\spiders\diaosi.py", line 47, in get_neirong
    next_page_url='https://m.diaosixs.org'+next_page
TypeError: must be str, not list
2019-08-06 21:49:17 [scrapy.core.scraper] ERROR: Spider error processing <GET https://m.diaosixs.org/1_1115/239734.html> (referer: https://m.diaosixs.org//1_1115/)
Traceback (most recent call last):
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "G:\pythonAI\爬虫大全\hbook\hbook\middlewares.py", line 35, in process_spider_output
    for i in result:
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "G:\pythonAI\爬虫大全\hbook\hbook\spiders\diaosi.py", line 47, in get_neirong
    next_page_url='https://m.diaosixs.org'+next_page
TypeError: must be str, not list
2019-08-06 21:49:17 [scrapy.core.scraper] ERROR: Spider error processing <GET https://m.diaosixs.org/1_1088/237732.html> (referer: https://m.diaosixs.org//1_1088/)
Traceback (most recent call last):
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "G:\pythonAI\爬虫大全\hbook\hbook\middlewares.py", line 35, in process_spider_output
    for i in result:
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "G:\pythonAI\爬虫大全\hbook\hbook\spiders\diaosi.py", line 47, in get_neirong
    next_page_url='https://m.diaosixs.org'+next_page
TypeError: must be str, not list
2019-08-06 21:49:17 [scrapy.core.scraper] ERROR: Spider error processing <GET https://m.diaosixs.org/1_1458/401523.html> (referer: https://m.diaosixs.org//1_1458/)
Traceback (most recent call last):
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "G:\pythonAI\爬虫大全\hbook\hbook\middlewares.py", line 35, in process_spider_output
    for i in result:
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "G:\pythonAI\爬虫大全\hbook\hbook\spiders\diaosi.py", line 47, in get_neirong
    next_page_url='https://m.diaosixs.org'+next_page
TypeError: must be str, not list
2019-08-06 21:49:17 [scrapy.core.scraper] ERROR: Spider error processing <GET https://m.diaosixs.org/0_14/5398.html> (referer: https://m.diaosixs.org//0_14/)
Traceback (most recent call last):
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "G:\pythonAI\爬虫大全\hbook\hbook\middlewares.py", line 35, in process_spider_output
    for i in result:
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "G:\pythonAI\爬虫大全\hbook\hbook\spiders\diaosi.py", line 47, in get_neirong
    next_page_url='https://m.diaosixs.org'+next_page
TypeError: must be str, not list
2019-08-06 21:49:17 [scrapy.core.scraper] ERROR: Spider error processing <GET https://m.diaosixs.org/1_1587/404213.html> (referer: https://m.diaosixs.org//1_1587/)
Traceback (most recent call last):
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "G:\pythonAI\爬虫大全\hbook\hbook\middlewares.py", line 35, in process_spider_output
    for i in result:
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "G:\pythonAI\爬虫大全\hbook\hbook\spiders\diaosi.py", line 47, in get_neirong
    next_page_url='https://m.diaosixs.org'+next_page
TypeError: must be str, not list
2019-08-06 21:49:17 [scrapy.core.scraper] ERROR: Spider error processing <GET https://m.diaosixs.org/0_571/116196.html> (referer: https://m.diaosixs.org//0_571/)
Traceback (most recent call last):
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "G:\pythonAI\爬虫大全\hbook\hbook\middlewares.py", line 35, in process_spider_output
    for i in result:
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "G:\pythonAI\爬虫大全\hbook\hbook\spiders\diaosi.py", line 47, in get_neirong
    next_page_url='https://m.diaosixs.org'+next_page
TypeError: must be str, not list
2019-08-06 21:49:17 [scrapy.core.scraper] ERROR: Spider error processing <GET https://m.diaosixs.org/0_572/116801.html> (referer: https://m.diaosixs.org//0_572/)
Traceback (most recent call last):
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "G:\pythonAI\爬虫大全\hbook\hbook\middlewares.py", line 35, in process_spider_output
    for i in result:
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "G:\pythonAI\爬虫大全\hbook\hbook\spiders\diaosi.py", line 47, in get_neirong
    next_page_url='https://m.diaosixs.org'+next_page
TypeError: must be str, not list
2019-08-06 21:49:17 [scrapy.core.scraper] ERROR: Spider error processing <GET https://m.diaosixs.org/0_1/1.html> (referer: https://m.diaosixs.org//0_1/)
Traceback (most recent call last):
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "G:\pythonAI\爬虫大全\hbook\hbook\middlewares.py", line 35, in process_spider_output
    for i in result:
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "G:\pythonAI\爬虫大全\hbook\hbook\spiders\diaosi.py", line 47, in get_neirong
    next_page_url='https://m.diaosixs.org'+next_page
TypeError: must be str, not list
2019-08-06 21:49:17 [scrapy.core.scraper] ERROR: Spider error processing <GET https://m.diaosixs.org/0_78/20304.html> (referer: https://m.diaosixs.org//0_78/)
Traceback (most recent call last):
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "G:\pythonAI\爬虫大全\hbook\hbook\middlewares.py", line 35, in process_spider_output
    for i in result:
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "G:\pythonAI\爬虫大全\hbook\hbook\spiders\diaosi.py", line 47, in get_neirong
    next_page_url='https://m.diaosixs.org'+next_page
TypeError: must be str, not list
2019-08-06 21:49:17 [scrapy.core.scraper] ERROR: Spider error processing <GET https://m.diaosixs.org/0_15/5579.html> (referer: https://m.diaosixs.org//0_15/)
Traceback (most recent call last):
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "G:\pythonAI\爬虫大全\hbook\hbook\middlewares.py", line 35, in process_spider_output
    for i in result:
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "G:\pythonAI\爬虫大全\hbook\hbook\spiders\diaosi.py", line 47, in get_neirong
    next_page_url='https://m.diaosixs.org'+next_page
TypeError: must be str, not list
2019-08-06 21:49:17 [scrapy.core.scraper] ERROR: Spider error processing <GET https://m.diaosixs.org/0_28/10773.html> (referer: https://m.diaosixs.org//0_28/)
Traceback (most recent call last):
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "G:\pythonAI\爬虫大全\hbook\hbook\middlewares.py", line 35, in process_spider_output
    for i in result:
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "G:\pythonAI\爬虫大全\hbook\hbook\spiders\diaosi.py", line 47, in get_neirong
    next_page_url='https://m.diaosixs.org'+next_page
TypeError: must be str, not list
2019-08-06 21:49:17 [scrapy.core.scraper] ERROR: Spider error processing <GET https://m.diaosixs.org/0_133/34344.html> (referer: https://m.diaosixs.org//0_133/)
Traceback (most recent call last):
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "G:\pythonAI\爬虫大全\hbook\hbook\middlewares.py", line 35, in process_spider_output
    for i in result:
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "G:\pythonAI\爬虫大全\hbook\hbook\spiders\diaosi.py", line 47, in get_neirong
    next_page_url='https://m.diaosixs.org'+next_page
TypeError: must be str, not list
2019-08-06 21:49:17 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-06 21:49:17 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 17924,
 'downloader/request_count': 42,
 'downloader/request_method_count/GET': 42,
 'downloader/response_bytes': 118995,
 'downloader/response_count': 42,
 'downloader/response_status_count/200': 41,
 'downloader/response_status_count/404': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 6, 13, 49, 17, 779693),
 'log_count/ERROR': 20,
 'log_count/INFO': 11,
 'log_count/WARNING': 1,
 'request_depth_max': 2,
 'response_received_count': 42,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/404': 1,
 'scheduler/dequeued': 41,
 'scheduler/dequeued/memory': 41,
 'scheduler/enqueued': 41,
 'scheduler/enqueued/memory': 41,
 'spider_exceptions/TypeError': 20,
 'start_time': datetime.datetime(2019, 8, 6, 13, 49, 12, 196235)}
2019-08-06 21:49:17 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-06 21:49:54 [scrapy.utils.log] INFO: Scrapy 1.6.0 started (bot: hbook)
2019-08-06 21:49:54 [scrapy.utils.log] INFO: Versions: lxml 4.3.4.0, libxml2 2.9.5, cssselect 1.0.3, parsel 1.5.1, w3lib 1.20.0, Twisted 18.4.0, Python 3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 17:00:18) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Windows-10-10.0.17763-SP0
2019-08-06 21:49:54 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hbook', 'LOG_FILE': 'hbook.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'hbook.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['hbook.spiders']}
2019-08-06 21:49:54 [scrapy.extensions.telnet] INFO: Telnet Password: 5090627a85334642
2019-08-06 21:49:54 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-06 21:49:55 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'hbook.rotate_useragent.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'hbook.middlewares.HbookDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-06 21:49:55 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'hbook.middlewares.HbookSpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-06 21:49:55 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-06 21:49:55 [scrapy.core.engine] INFO: Spider opened
2019-08-06 21:49:55 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-06 21:49:55 [diaosi] INFO: Spider opened: diaosi
2019-08-06 21:49:55 [py.warnings] WARNING: C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\offsite.py:61: URLWarning: allowed_domains accepts only domains, not URLs. Ignoring URL entry https://m.diaosixs.org in allowed_domains.
  warnings.warn(message, URLWarning)

2019-08-06 21:49:55 [diaosi] INFO: Spider opened: diaosi
2019-08-06 21:49:55 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-06 21:49:59 [scrapy.core.scraper] ERROR: Spider error processing <GET https://m.diaosixs.org/1_1115/239734.html> (referer: https://m.diaosixs.org//1_1115/)
Traceback (most recent call last):
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "G:\pythonAI\爬虫大全\hbook\hbook\spiders\diaosi.py", line 47, in get_neirong
    next_page_url='https://m.diaosixs.org'+next_page
TypeError: must be str, not list
2019-08-06 21:49:59 [scrapy.core.scraper] ERROR: Spider error processing <GET https://m.diaosixs.org/0_571/116196.html> (referer: https://m.diaosixs.org//0_571/)
Traceback (most recent call last):
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "G:\pythonAI\爬虫大全\hbook\hbook\spiders\diaosi.py", line 47, in get_neirong
    next_page_url='https://m.diaosixs.org'+next_page
TypeError: must be str, not list
2019-08-06 21:49:59 [scrapy.core.scraper] ERROR: Spider error processing <GET https://m.diaosixs.org/1_1088/237732.html> (referer: https://m.diaosixs.org//1_1088/)
Traceback (most recent call last):
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "G:\pythonAI\爬虫大全\hbook\hbook\spiders\diaosi.py", line 47, in get_neirong
    next_page_url='https://m.diaosixs.org'+next_page
TypeError: must be str, not list
2019-08-06 21:49:59 [scrapy.core.scraper] ERROR: Spider error processing <GET https://m.diaosixs.org/0_32/11144.html> (referer: https://m.diaosixs.org//0_32/)
Traceback (most recent call last):
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "G:\pythonAI\爬虫大全\hbook\hbook\spiders\diaosi.py", line 47, in get_neirong
    next_page_url='https://m.diaosixs.org'+next_page
TypeError: must be str, not list
2019-08-06 21:49:59 [scrapy.core.scraper] ERROR: Spider error processing <GET https://m.diaosixs.org/1_1458/401523.html> (referer: https://m.diaosixs.org//1_1458/)
Traceback (most recent call last):
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "G:\pythonAI\爬虫大全\hbook\hbook\spiders\diaosi.py", line 47, in get_neirong
    next_page_url='https://m.diaosixs.org'+next_page
TypeError: must be str, not list
2019-08-06 21:49:59 [scrapy.core.scraper] ERROR: Spider error processing <GET https://m.diaosixs.org/0_46/13998.html> (referer: https://m.diaosixs.org//0_46/)
Traceback (most recent call last):
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "G:\pythonAI\爬虫大全\hbook\hbook\spiders\diaosi.py", line 47, in get_neirong
    next_page_url='https://m.diaosixs.org'+next_page
TypeError: must be str, not list
2019-08-06 21:49:59 [scrapy.core.scraper] ERROR: Spider error processing <GET https://m.diaosixs.org/0_742/151701.html> (referer: https://m.diaosixs.org//0_742/)
Traceback (most recent call last):
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "G:\pythonAI\爬虫大全\hbook\hbook\spiders\diaosi.py", line 47, in get_neirong
    next_page_url='https://m.diaosixs.org'+next_page
TypeError: must be str, not list
2019-08-06 21:49:59 [scrapy.core.scraper] ERROR: Spider error processing <GET https://m.diaosixs.org/0_14/5398.html> (referer: https://m.diaosixs.org//0_14/)
Traceback (most recent call last):
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "G:\pythonAI\爬虫大全\hbook\hbook\spiders\diaosi.py", line 47, in get_neirong
    next_page_url='https://m.diaosixs.org'+next_page
TypeError: must be str, not list
2019-08-06 21:50:00 [scrapy.core.scraper] ERROR: Spider error processing <GET https://m.diaosixs.org/0_354/73861.html> (referer: https://m.diaosixs.org//0_354/)
Traceback (most recent call last):
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "G:\pythonAI\爬虫大全\hbook\hbook\spiders\diaosi.py", line 47, in get_neirong
    next_page_url='https://m.diaosixs.org'+next_page
TypeError: must be str, not list
2019-08-06 21:50:00 [scrapy.core.scraper] ERROR: Spider error processing <GET https://m.diaosixs.org/0_28/10773.html> (referer: https://m.diaosixs.org//0_28/)
Traceback (most recent call last):
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "G:\pythonAI\爬虫大全\hbook\hbook\spiders\diaosi.py", line 47, in get_neirong
    next_page_url='https://m.diaosixs.org'+next_page
TypeError: must be str, not list
2019-08-06 21:50:00 [scrapy.core.scraper] ERROR: Spider error processing <GET https://m.diaosixs.org/0_655/138662.html> (referer: https://m.diaosixs.org//0_655/)
Traceback (most recent call last):
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "G:\pythonAI\爬虫大全\hbook\hbook\spiders\diaosi.py", line 47, in get_neirong
    next_page_url='https://m.diaosixs.org'+next_page
TypeError: must be str, not list
2019-08-06 21:50:00 [scrapy.core.scraper] ERROR: Spider error processing <GET https://m.diaosixs.org/0_15/5579.html> (referer: https://m.diaosixs.org//0_15/)
Traceback (most recent call last):
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "G:\pythonAI\爬虫大全\hbook\hbook\spiders\diaosi.py", line 47, in get_neirong
    next_page_url='https://m.diaosixs.org'+next_page
TypeError: must be str, not list
2019-08-06 21:50:00 [scrapy.core.scraper] ERROR: Spider error processing <GET https://m.diaosixs.org/0_378/76550.html> (referer: https://m.diaosixs.org//0_378/)
Traceback (most recent call last):
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "G:\pythonAI\爬虫大全\hbook\hbook\spiders\diaosi.py", line 47, in get_neirong
    next_page_url='https://m.diaosixs.org'+next_page
TypeError: must be str, not list
2019-08-06 21:50:00 [scrapy.core.scraper] ERROR: Spider error processing <GET https://m.diaosixs.org/0_572/116801.html> (referer: https://m.diaosixs.org//0_572/)
Traceback (most recent call last):
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "G:\pythonAI\爬虫大全\hbook\hbook\spiders\diaosi.py", line 47, in get_neirong
    next_page_url='https://m.diaosixs.org'+next_page
TypeError: must be str, not list
2019-08-06 21:50:00 [scrapy.core.scraper] ERROR: Spider error processing <GET https://m.diaosixs.org/0_27/10469.html> (referer: https://m.diaosixs.org//0_27/)
Traceback (most recent call last):
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "G:\pythonAI\爬虫大全\hbook\hbook\spiders\diaosi.py", line 47, in get_neirong
    next_page_url='https://m.diaosixs.org'+next_page
TypeError: must be str, not list
2019-08-06 21:50:01 [scrapy.core.scraper] ERROR: Spider error processing <GET https://m.diaosixs.org/1_1587/404213.html> (referer: https://m.diaosixs.org//1_1587/)
Traceback (most recent call last):
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "G:\pythonAI\爬虫大全\hbook\hbook\spiders\diaosi.py", line 47, in get_neirong
    next_page_url='https://m.diaosixs.org'+next_page
TypeError: must be str, not list
2019-08-06 21:50:01 [scrapy.core.scraper] ERROR: Spider error processing <GET https://m.diaosixs.org/0_78/20304.html> (referer: https://m.diaosixs.org//0_78/)
Traceback (most recent call last):
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "G:\pythonAI\爬虫大全\hbook\hbook\spiders\diaosi.py", line 47, in get_neirong
    next_page_url='https://m.diaosixs.org'+next_page
TypeError: must be str, not list
2019-08-06 21:50:01 [scrapy.core.scraper] ERROR: Spider error processing <GET https://m.diaosixs.org/0_132/34330.html> (referer: https://m.diaosixs.org//0_132/)
Traceback (most recent call last):
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "G:\pythonAI\爬虫大全\hbook\hbook\spiders\diaosi.py", line 47, in get_neirong
    next_page_url='https://m.diaosixs.org'+next_page
TypeError: must be str, not list
2019-08-06 21:50:01 [scrapy.core.scraper] ERROR: Spider error processing <GET https://m.diaosixs.org/0_1/1.html> (referer: https://m.diaosixs.org//0_1/)
Traceback (most recent call last):
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "G:\pythonAI\爬虫大全\hbook\hbook\spiders\diaosi.py", line 47, in get_neirong
    next_page_url='https://m.diaosixs.org'+next_page
TypeError: must be str, not list
2019-08-06 21:50:01 [scrapy.core.scraper] ERROR: Spider error processing <GET https://m.diaosixs.org/0_133/34344.html> (referer: https://m.diaosixs.org//0_133/)
Traceback (most recent call last):
  File "C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "G:\pythonAI\爬虫大全\hbook\hbook\spiders\diaosi.py", line 47, in get_neirong
    next_page_url='https://m.diaosixs.org'+next_page
TypeError: must be str, not list
2019-08-06 21:50:01 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-06 21:50:01 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 18222,
 'downloader/request_count': 42,
 'downloader/request_method_count/GET': 42,
 'downloader/response_bytes': 118947,
 'downloader/response_count': 42,
 'downloader/response_status_count/200': 41,
 'downloader/response_status_count/404': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 6, 13, 50, 1, 876852),
 'log_count/ERROR': 20,
 'log_count/INFO': 11,
 'log_count/WARNING': 1,
 'request_depth_max': 2,
 'response_received_count': 42,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/404': 1,
 'scheduler/dequeued': 41,
 'scheduler/dequeued/memory': 41,
 'scheduler/enqueued': 41,
 'scheduler/enqueued/memory': 41,
 'spider_exceptions/TypeError': 20,
 'start_time': datetime.datetime(2019, 8, 6, 13, 49, 55, 720830)}
2019-08-06 21:50:01 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-06 21:50:51 [scrapy.utils.log] INFO: Scrapy 1.6.0 started (bot: hbook)
2019-08-06 21:50:51 [scrapy.utils.log] INFO: Versions: lxml 4.3.4.0, libxml2 2.9.5, cssselect 1.0.3, parsel 1.5.1, w3lib 1.20.0, Twisted 18.4.0, Python 3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 17:00:18) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Windows-10-10.0.17763-SP0
2019-08-06 21:50:51 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hbook', 'LOG_FILE': 'hbook.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'hbook.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['hbook.spiders']}
2019-08-06 21:50:51 [scrapy.extensions.telnet] INFO: Telnet Password: 90fba9512fcb7355
2019-08-06 21:50:51 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-06 21:50:52 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'hbook.rotate_useragent.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'hbook.middlewares.HbookDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-06 21:50:52 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'hbook.middlewares.HbookSpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-06 21:50:52 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-06 21:50:52 [scrapy.core.engine] INFO: Spider opened
2019-08-06 21:50:52 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-06 21:50:52 [diaosi] INFO: Spider opened: diaosi
2019-08-06 21:50:52 [py.warnings] WARNING: C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\offsite.py:61: URLWarning: allowed_domains accepts only domains, not URLs. Ignoring URL entry https://m.diaosixs.org in allowed_domains.
  warnings.warn(message, URLWarning)

2019-08-06 21:50:52 [diaosi] INFO: Spider opened: diaosi
2019-08-06 21:50:52 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-06 21:50:57 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-06 21:50:57 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 17909,
 'downloader/request_count': 42,
 'downloader/request_method_count/GET': 42,
 'downloader/response_bytes': 119013,
 'downloader/response_count': 42,
 'downloader/response_status_count/200': 41,
 'downloader/response_status_count/404': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 6, 13, 50, 57, 570128),
 'log_count/INFO': 11,
 'log_count/WARNING': 1,
 'request_depth_max': 2,
 'response_received_count': 42,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/404': 1,
 'scheduler/dequeued': 41,
 'scheduler/dequeued/memory': 41,
 'scheduler/enqueued': 41,
 'scheduler/enqueued/memory': 41,
 'start_time': datetime.datetime(2019, 8, 6, 13, 50, 52, 375993)}
2019-08-06 21:50:57 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-06 21:52:06 [scrapy.utils.log] INFO: Scrapy 1.6.0 started (bot: hbook)
2019-08-06 21:52:06 [scrapy.utils.log] INFO: Versions: lxml 4.3.4.0, libxml2 2.9.5, cssselect 1.0.3, parsel 1.5.1, w3lib 1.20.0, Twisted 18.4.0, Python 3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 17:00:18) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Windows-10-10.0.17763-SP0
2019-08-06 21:52:06 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hbook', 'LOG_FILE': 'hbook.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'hbook.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['hbook.spiders']}
2019-08-06 21:52:06 [scrapy.extensions.telnet] INFO: Telnet Password: 9bf924c1b3278bc7
2019-08-06 21:52:06 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-06 21:52:07 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'hbook.rotate_useragent.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'hbook.middlewares.HbookDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-06 21:52:07 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'hbook.middlewares.HbookSpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-06 21:52:07 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-06 21:52:07 [scrapy.core.engine] INFO: Spider opened
2019-08-06 21:52:07 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-06 21:52:07 [diaosi] INFO: Spider opened: diaosi
2019-08-06 21:52:07 [py.warnings] WARNING: C:\Users\ltf\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\spidermiddlewares\offsite.py:61: URLWarning: allowed_domains accepts only domains, not URLs. Ignoring URL entry https://m.diaosixs.org in allowed_domains.
  warnings.warn(message, URLWarning)

2019-08-06 21:52:07 [diaosi] INFO: Spider opened: diaosi
2019-08-06 21:52:07 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-06 21:52:11 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-06 21:52:11 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 18236,
 'downloader/request_count': 42,
 'downloader/request_method_count/GET': 42,
 'downloader/response_bytes': 119120,
 'downloader/response_count': 42,
 'downloader/response_status_count/200': 41,
 'downloader/response_status_count/404': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 6, 13, 52, 11, 884031),
 'log_count/INFO': 11,
 'log_count/WARNING': 1,
 'offsite/domains': 1,
 'offsite/filtered': 20,
 'request_depth_max': 3,
 'response_received_count': 42,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/404': 1,
 'scheduler/dequeued': 41,
 'scheduler/dequeued/memory': 41,
 'scheduler/enqueued': 41,
 'scheduler/enqueued/memory': 41,
 'start_time': datetime.datetime(2019, 8, 6, 13, 52, 7, 789572)}
2019-08-06 21:52:11 [scrapy.core.engine] INFO: Spider closed (finished)
